# Epic 4.5: LLM-First Code Generation Refactor

**Status**: Not Started
**Priority**: Critical
**Epic Owner**: Backend/AI Team
**Estimated Tasks**: 15
**Depends On**: Epic 4 (Current Code Generation)
**Timeline**: 4-5 weeks

---

## Overview

Replace the current fragmented 8-stage template-based pipeline with a simplified 3-stage LLM-first architecture that generates coherent, high-quality component code through intelligent generation and validation.

**Current Problem**: The existing pipeline uses regex-based string manipulation across 8 disconnected stages, resulting in:
- Fragmented, incoherent code with duplicate types
- Low TypeScript compilation success rate (~60%)
- Poor integration of requirements, tokens, and accessibility
- Complex, hard-to-maintain codebase (~2800 LOC)
- Brittle regex patterns that break easily

**Solution**: Simplify to 3-stage LLM-first pipeline:
1. **Generate** - Single LLM call creates complete, coherent component
2. **Validate** - TypeScript/ESLint validation with LLM-based fixes
3. **Post-Process** - Import resolution, formatting, provenance

---

## Goals

1. **Improve code quality**: 60% → 95%+ TypeScript compilation success
2. **Simplify architecture**: 8 stages → 3 stages
3. **Reduce complexity**: Delete 5 overengineered modules, add 3 focused modules
4. **Enable self-healing**: Validation loop with LLM-based error correction
5. **Maintain traceability**: Keep provenance tracking and LangSmith observability
6. **Reduce codebase**: ~2800 LOC → ~1500 LOC
7. **Improve latency**: p50 60s → 30s, p95 90s → 60s

---

## Success Criteria

- ✅ TypeScript compilation success rate ≥95%
- ✅ Code quality score 8-9/10 (from 5/10)
- ✅ Generation latency p50 <30s (from ~60s)
- ✅ Generation latency p95 <60s (from ~90s)
- ✅ User regeneration rate <10% (from high)
- ✅ Test coverage >90%
- ✅ Codebase reduced by ~50%
- ✅ All tests passing
- ✅ Production deployment successful

---

## Architecture Comparison

### Before (Current - 8 Stages)
```
Pattern JSON → Parse Pattern
              ↓
          Token Injection (regex)
              ↓
          Tailwind Generation (templates)
              ↓
          Requirement Implementation (regex)
              ↓
          A11y Enhancement (regex)
              ↓
          Type Generation (regex)
              ↓
          Storybook Generation (templates)
              ↓
          Code Assembly (concatenation)
              ↓
          Prettier Formatting
              ↓
          Output (often broken)
```

**Problems**:
- Each stage operates independently without context
- Stages conflict (duplicate types, conflicting ARIA)
- Regex manipulation is fragile and error-prone
- No validation before returning to user
- Code fragments don't integrate well

### After (LLM-First - 3 Stages)
```
Pattern JSON (reference) + Tokens + Requirements
              ↓
          LLM Generation (single pass)
          - Complete component code
          - Integrated types, props, events
          - Built-in accessibility
          - Storybook stories
              ↓
          Validation Loop
          - TypeScript compilation check
          - ESLint validation
          - If errors: LLM fixes (max 2 retries)
              ↓
          Post-Processing
          - Import resolution & ordering
          - Provenance header injection
          - Prettier formatting
              ↓
          Output (validated, working code)
```

**Benefits**:
- Single coherent generation with full context
- Semantic understanding of requirements
- Self-healing through validation loop
- Guaranteed valid output
- Much simpler to maintain and improve

---

## Tasks

### **Task 1: Build LLM Code Generator (Core)** ⭐
**Priority**: P0 (Critical Path)
**Estimated Time**: 3-4 days

**Create**: `backend/src/generation/llm_generator.py`

**Acceptance Criteria**:
- [ ] `LLMComponentGenerator` class with structured output
- [ ] Comprehensive prompt builder that includes:
  - Pattern code as reference example
  - Design tokens with semantic descriptions
  - All requirements (props, events, states, a11y)
  - Component naming and conventions
  - TypeScript strict mode requirements
  - Accessibility requirements
  - shadcn/ui style guidelines
- [ ] Use OpenAI structured outputs or JSON mode
- [ ] Generate complete component code (not fragments)
- [ ] Generate Storybook stories in same call
- [ ] LangSmith tracing for all LLM calls
- [ ] Error handling with retries (3 attempts)
- [ ] Response validation (check for required fields)
- [ ] Token usage tracking and optimization

**Output Schema**:
```python
@dataclass
class LLMGeneratedCode:
    component_code: str      # Complete .tsx file content
    stories_code: str        # Complete .stories.tsx content
    imports: List[str]       # List of import statements
    exports: List[str]       # List of exported names
    explanation: str         # Why code was generated this way (for debugging)
    token_usage: Dict        # Prompt/completion tokens used
```

**Prompt Structure**:
```
System: You are an expert React/TypeScript developer...

User:
## Task
Generate a production-ready React component based on the requirements below.

## Reference Pattern (shadcn/ui)
[Pattern code as example]

## Design Tokens
Colors: {...}
Typography: {...}
Spacing: {...}

## Requirements
Props: [{name, type, description}]
Events: [{name, signature}]
States: [{name, type, default}]
Accessibility: [{requirement}]

## Constraints
- TypeScript strict mode (no 'any' types)
- Include proper ARIA attributes
- Use design tokens via CSS variables
- Follow shadcn/ui conventions
- Export component with displayName

## Output Format
Return JSON with: component_code, stories_code, imports, exports, explanation
```

**Tests**:
- Test prompt construction
- Test structured output parsing
- Test error handling
- Test token usage tracking
- Test with various component types

**Dependencies**: OpenAI API, LangSmith

---

### **Task 2: Build Code Validator with Fix Loop** ⭐
**Priority**: P0 (Critical Path)
**Estimated Time**: 3-4 days

**Create**: `backend/src/generation/code_validator.py`

**Acceptance Criteria**:
- [ ] TypeScript validation using `ts.createProgram` API (via Node.js subprocess)
- [ ] ESLint validation via Node.js subprocess
- [ ] Validation result parsing (errors, warnings, line numbers)
- [ ] LLM-based fix generator that:
  - Takes original code + validation errors
  - Generates fixed code with explanation
  - Preserves working parts
  - Only fixes specific issues
- [ ] Iterative fix loop (max 2-3 attempts)
- [ ] Quality scoring:
  - Compilation success: boolean
  - Lint error count: int
  - Lint warning count: int
  - Type coverage: percentage
- [ ] Performance tracking (<5s validation time)
- [ ] Graceful degradation if validation unavailable
- [ ] Parallel validation (TypeScript + ESLint simultaneously)

**Implementation**:
```python
class CodeValidator:
    async def validate_and_fix(
        self,
        code: str,
        max_retries: int = 2
    ) -> ValidationResult:
        """Validate code, use LLM to fix if needed."""

        for attempt in range(max_retries + 1):
            # Run validations in parallel
            ts_result, lint_result = await asyncio.gather(
                self._validate_typescript(code),
                self._validate_eslint(code)
            )

            if ts_result.valid and lint_result.valid:
                return ValidationResult(
                    valid=True,
                    code=code,
                    attempts=attempt,
                    quality_score=self._calculate_quality(ts_result, lint_result)
                )

            if attempt < max_retries:
                # Use LLM to fix errors
                code = await self._llm_fix_errors(
                    code,
                    ts_result.errors,
                    lint_result.errors
                )

        return ValidationResult(
            valid=False,
            code=code,
            attempts=max_retries,
            errors=[...ts_result.errors, ...lint_result.errors]
        )
```

**Fix Prompt Strategy**:
```
You are debugging TypeScript/ESLint errors in React component code.

## Original Code
[code]

## Validation Errors
TypeScript: [errors with line numbers]
ESLint: [errors with line numbers]

## Task
Fix ONLY the specific errors listed above. Preserve all working code.
Return the complete fixed code.
```

**Tests**:
- Test TypeScript validation (valid/invalid code)
- Test ESLint validation
- Test fix generation
- Test fix loop convergence
- Test quality scoring
- Test parallel validation performance

**Dependencies**: Node.js, TypeScript, ESLint, OpenAI API

---

### **Task 3: Build Prompt Builder** ⭐
**Priority**: P0 (Critical Path)
**Estimated Time**: 2-3 days

**Create**: `backend/src/generation/prompt_builder.py`

**Acceptance Criteria**:
- [ ] `PromptBuilder` class with template management
- [ ] System prompt for component generation
- [ ] Dynamic prompt construction from:
  - Pattern reference (format as example)
  - Design tokens (explain semantic meaning)
  - Requirements (structured by category)
  - Component name and type
  - Exemplars (few-shot learning)
- [ ] Few-shot examples library:
  - Load 2-3 relevant exemplars based on component type
  - Include input (requirements) and output (code)
- [ ] Anti-pattern examples (what NOT to do):
  - Using `any` types
  - Missing ARIA attributes
  - Hardcoded colors instead of tokens
- [ ] Token usage optimization:
  - Truncate long patterns if needed
  - Compress requirements representation
  - Target: <8000 tokens per prompt
- [ ] Prompt versioning for A/B testing
- [ ] Validation constraints in prompt:
  - Zero `any` types
  - All props must have types
  - ARIA attributes required
  - Proper import structure
  - Component displayName required

**Implementation**:
```python
class PromptBuilder:
    def __init__(self, exemplar_loader: ExemplarLoader):
        self.exemplar_loader = exemplar_loader
        self.system_prompt = self._load_system_prompt()

    def build_generation_prompt(
        self,
        pattern: PatternStructure,
        tokens: Dict,
        requirements: Dict,
        component_name: str
    ) -> List[Message]:
        """Build complete prompt for generation."""

        # Select relevant exemplars
        exemplars = self.exemplar_loader.select_exemplars(
            component_type=pattern.component_type,
            num_examples=2
        )

        # Build user message
        user_message = f"""
## Task
Generate a production-ready {component_name} component.

## Reference Pattern (shadcn/ui)
{self._format_pattern(pattern)}

## Design Tokens
{self._format_tokens(tokens)}

## Requirements
{self._format_requirements(requirements)}

## Examples
{self._format_exemplars(exemplars)}

## Anti-Patterns (Avoid)
- Using 'any' types
- Missing ARIA attributes
- Hardcoded color values

## Output Format
Return JSON with: component_code, stories_code, imports, exports, explanation
"""

        return [
            {"role": "system", "content": self.system_prompt},
            {"role": "user", "content": user_message}
        ]
```

**Tests**:
- Test prompt construction
- Test token counting
- Test exemplar selection
- Test prompt versioning
- Test with various component types

---

### **Task 4: Create Exemplar Library**
**Priority**: P1 (Required for quality)
**Estimated Time**: 2-3 days

**Create**:
- `backend/data/exemplars/` directory
- `backend/src/generation/exemplar_loader.py`

**Acceptance Criteria**:
- [ ] 5 hand-crafted exemplar components:
  - **Button**: variants, sizes, accessibility, icons
  - **Card**: composition, semantic HTML, proper structure
  - **Input**: validation, error states, labels, hints
  - **Checkbox**: controlled, accessibility, labels
  - **Alert**: ARIA live regions, icons, variants
- [ ] Each exemplar includes:
  - `input.json`: Requirements and tokens
  - `output.tsx`: Perfect generated component
  - `output.stories.tsx`: Storybook stories
  - `metadata.json`: Why this is a good example
  - `comments.md`: Explain key decisions
- [ ] JSON metadata for each exemplar:
  ```json
  {
    "component_type": "button",
    "demonstrates": [
      "variant prop with union types",
      "proper ARIA attributes",
      "forwardRef pattern",
      "displayName usage",
      "CVA for variants"
    ],
    "tokens_used": ["colors", "spacing", "typography"],
    "quality_score": 95
  }
  ```
- [ ] `ExemplarLoader` class:
  - Load exemplars from disk
  - Select relevant exemplars by component type
  - Format for prompt inclusion
  - Cache loaded exemplars
- [ ] Selection logic:
  - Prefer same component type (Button → Button)
  - Fall back to similar types (Checkbox → Switch)
  - Limit to 2-3 examples per prompt (token budget)

**Exemplar Structure**:
```
backend/data/exemplars/
├── button/
│   ├── input.json
│   ├── output.tsx
│   ├── output.stories.tsx
│   ├── metadata.json
│   └── README.md
├── card/
│   └── ...
└── input/
    └── ...
```

**Tests**:
- Test exemplar loading
- Test selection logic
- Test formatting for prompts
- Test caching

---

### **Task 5: Refactor Generator Service** ⭐
**Priority**: P0 (Critical Path)
**Estimated Time**: 2-3 days

**Update**: `backend/src/generation/generator_service.py`

**Acceptance Criteria**:
- [ ] Remove all old stage calls:
  - ❌ `_inject_tokens()`
  - ❌ `_generate_tailwind()`
  - ❌ `_implement_requirements()`
  - ❌ `_enhance_accessibility()`
  - ❌ `_generate_types()`
  - ❌ `_generate_storybook_stories()`
- [ ] New simplified pipeline:
  1. **Load pattern** (as reference only)
  2. **Build generation prompt** (with exemplars)
  3. **Call LLM generator** (single pass)
  4. **Validate generated code** (TypeScript + ESLint)
  5. **If invalid**: LLM fix loop (max 2 retries)
  6. **Post-process**: imports, provenance, formatting
  7. **Return result** with metadata
- [ ] Stage tracking updated to 3 stages:
  - `GENERATING`: LLM generation (~15-20s)
  - `VALIDATING`: TypeScript/ESLint validation (~3-5s)
  - `POST_PROCESSING`: Final assembly (~2-3s)
- [ ] Preserve LangSmith tracing:
  - Trace LLM calls
  - Trace validation
  - Trace fix attempts
- [ ] Detailed error reporting:
  - LLM generation errors
  - Validation errors (with line numbers)
  - Fix attempt history
- [ ] Performance metrics:
  - LLM latency (prompt + completion)
  - Validation latency
  - Fix loop latency
  - Total latency (target: <30s p50, <60s p95)
- [ ] Quality metrics in metadata:
  - Validation attempts (0 = perfect first try)
  - Final validation status (pass/fail)
  - Quality scores (compilation, linting, type safety)
  - Token usage

**Implementation**:
```python
class GeneratorService:
    @traceable(run_type="chain", name="generate_component_llm_first")
    async def generate(self, request: GenerationRequest) -> GenerationResult:
        """Generate component using LLM-first approach."""
        start_time = time.time()

        try:
            # Stage 1: Load pattern as reference
            pattern = await self._load_pattern(request.pattern_id)

            # Stage 2: Build prompt with exemplars
            prompt = self.prompt_builder.build_generation_prompt(
                pattern=pattern,
                tokens=request.tokens,
                requirements=request.requirements,
                component_name=request.component_name
            )

            # Stage 3: Generate with LLM
            self.current_stage = GenerationStage.GENERATING
            generated = await self.llm_generator.generate(prompt)

            # Stage 4: Validate and fix if needed
            self.current_stage = GenerationStage.VALIDATING
            validated = await self.code_validator.validate_and_fix(
                generated.component_code,
                max_retries=2
            )

            if not validated.valid:
                # Return with detailed errors
                return GenerationResult(
                    success=False,
                    error="Code validation failed after 2 fix attempts",
                    validation_errors=validated.errors,
                    metadata=self._build_metadata(start_time, generated, validated)
                )

            # Stage 5: Post-process
            self.current_stage = GenerationStage.POST_PROCESSING
            final_code = await self._post_process(
                validated.code,
                generated.stories_code,
                request.pattern_id,
                request.tokens,
                request.requirements
            )

            # Build successful result
            return GenerationResult(
                success=True,
                component_code=final_code["component"],
                stories_code=final_code["stories"],
                files=final_code["files"],
                metadata=self._build_metadata(start_time, generated, validated)
            )

        except Exception as e:
            logger.error(f"Generation failed: {e}")
            return GenerationResult(
                success=False,
                error=str(e),
                metadata=self._build_metadata(start_time)
            )
```

**Tests**:
- Test successful generation path
- Test validation failure path
- Test fix loop convergence
- Test error handling
- Test performance metrics
- Test LangSmith tracing

---

### **Task 6: Update Pattern Parser (Simplify)**
**Priority**: P1
**Estimated Time**: 1 day

**Update**: `backend/src/generation/pattern_parser.py`

**Acceptance Criteria**:
- [ ] Remove modification point detection (no longer needed):
  - ❌ `_find_modification_points()`
  - ❌ className location tracking
  - ❌ variant definition tracking
- [ ] Remove regex-based code analysis:
  - ❌ `_extract_props_interface()`
  - ❌ `_extract_imports()` (handled by ImportResolver)
- [ ] Keep only basic parsing:
  - ✅ `load_pattern(pattern_id)` - Load JSON
  - ✅ Extract metadata (name, type, variants)
  - ✅ Return pattern code as string (reference only)
- [ ] Add pattern metadata extraction:
  - Component type (button, card, input, etc.)
  - Variants list (for prompt context)
  - Dependencies list
  - A11y features
- [ ] Return simplified `PatternStructure`:
  ```python
  @dataclass
  class PatternStructure:
      component_name: str
      component_type: str
      code: str  # Complete pattern code (reference)
      variants: List[str]
      dependencies: List[str]
      metadata: Dict[str, Any]
  ```
- [ ] Performance: <50ms per pattern
- [ ] Error handling for missing patterns

**Tests**:
- Test pattern loading
- Test metadata extraction
- Test error handling
- Test performance

---

### **Task 7: Simplify Code Assembler**
**Priority**: P1
**Estimated Time**: 1 day

**Update**: `backend/src/generation/code_assembler.py`

**Acceptance Criteria**:
- [ ] Remove multi-section assembly logic (no longer needed):
  - ❌ `_build_code_parts()` from multiple stages
  - ❌ Type definitions section
  - ❌ CSS variables section (now in code)
  - ❌ Component code section
- [ ] Keep only:
  - ✅ **Provenance header injection** (delegate to ProvenanceGenerator)
  - ✅ **Import resolution** (delegate to ImportResolver)
  - ✅ **Prettier formatting** (via Node.js script)
- [ ] Simplified interface:
  ```python
  class CodeAssembler:
      async def assemble(
          self,
          component_code: str,  # Complete code from LLM
          stories_code: str,     # Complete stories from LLM
          pattern_id: str,
          tokens: Dict,
          requirements: Dict,
          component_name: str
      ) -> AssembledCode:
          """Assemble final code with header, imports, formatting."""

          # Add provenance header
          component_with_header = self.provenance_generator.inject_header(
              component_code,
              pattern_id,
              tokens,
              requirements
          )

          # Resolve and order imports
          component_with_imports = self.import_resolver.resolve_and_order(
              component_with_header
          )

          # Format with Prettier
          formatted_component = await self._format_code(component_with_imports)
          formatted_stories = await self._format_code(stories_code)

          return AssembledCode(
              component=formatted_component,
              stories=formatted_stories,
              files={
                  f"{component_name}.tsx": formatted_component,
                  f"{component_name}.stories.tsx": formatted_stories
              }
          )
  ```
- [ ] Input: complete component code string (not fragments)
- [ ] Output: formatted code with header and organized imports
- [ ] Performance: <2s for formatting

**Tests**:
- Test header injection
- Test import resolution
- Test formatting
- Test error handling

---

### **Task 8: Add Validation Scripts (Node.js)** ⭐
**Priority**: P0 (Required for Task 2)
**Estimated Time**: 1-2 days

**Create**:
- `backend/scripts/validate_typescript.js`
- `backend/scripts/validate_eslint.js`

**Acceptance Criteria**:

**TypeScript Validation Script**:
- [ ] Accepts code via stdin
- [ ] Creates temp file in `.tmp/` directory
- [ ] Runs `ts.createProgram` with strict mode config
- [ ] Returns JSON with errors/warnings:
  ```json
  {
    "valid": false,
    "errors": [
      {
        "line": 42,
        "column": 10,
        "message": "Type 'string' is not assignable to type 'number'",
        "code": "TS2322"
      }
    ],
    "warnings": []
  }
  ```
- [ ] Cleanup temp files
- [ ] Exit codes: 0 (valid), 1 (errors), 2 (fatal)
- [ ] Performance: <2s for typical component

**ESLint Validation Script**:
- [ ] Accepts code via stdin
- [ ] Runs ESLint programmatically (no temp file needed)
- [ ] Use existing `app/eslint.config.mjs` configuration
- [ ] Returns JSON with errors/warnings:
  ```json
  {
    "valid": false,
    "errors": [
      {
        "line": 15,
        "column": 5,
        "message": "Missing return type on function",
        "ruleId": "@typescript-eslint/explicit-function-return-type"
      }
    ],
    "warnings": []
  }
  ```
- [ ] Exit codes: 0 (valid), 1 (errors), 2 (fatal)
- [ ] Performance: <2s for typical component

**Both scripts handle edge cases**:
- [ ] Malformed code (syntax errors)
- [ ] Missing dependencies (graceful degradation)
- [ ] Timeout after 10s
- [ ] Proper error messages

**Implementation** (`validate_typescript.js`):
```javascript
#!/usr/bin/env node

const ts = require('typescript');
const fs = require('fs');
const path = require('path');

async function validateTypeScript(code) {
  // Create temp file
  const tmpDir = path.join(__dirname, '../.tmp');
  if (!fs.existsSync(tmpDir)) {
    fs.mkdirSync(tmpDir, { recursive: true });
  }

  const tmpFile = path.join(tmpDir, `validate-${Date.now()}.tsx`);
  fs.writeFileSync(tmpFile, code);

  try {
    // Load tsconfig
    const configPath = path.join(__dirname, '../../app/tsconfig.json');
    const configFile = ts.readConfigFile(configPath, ts.sys.readFile);
    const parsedConfig = ts.parseJsonConfigFileContent(
      configFile.config,
      ts.sys,
      path.dirname(configPath)
    );

    // Create program and get diagnostics
    const program = ts.createProgram([tmpFile], parsedConfig.options);
    const diagnostics = ts.getPreEmitDiagnostics(program);

    // Format results
    const errors = [];
    const warnings = [];

    for (const diagnostic of diagnostics) {
      const message = ts.flattenDiagnosticMessageText(diagnostic.messageText, '\n');
      let line = 0, column = 0;

      if (diagnostic.file && diagnostic.start !== undefined) {
        const { line: lineNum, character } = diagnostic.file.getLineAndCharacterOfPosition(diagnostic.start);
        line = lineNum + 1;
        column = character + 1;
      }

      const issue = {
        line,
        column,
        message,
        code: `TS${diagnostic.code}`
      };

      if (diagnostic.category === ts.DiagnosticCategory.Error) {
        errors.push(issue);
      } else if (diagnostic.category === ts.DiagnosticCategory.Warning) {
        warnings.push(issue);
      }
    }

    // Output JSON
    const result = {
      valid: errors.length === 0,
      errors,
      warnings
    };

    console.log(JSON.stringify(result, null, 2));

    // Exit code
    process.exit(errors.length > 0 ? 1 : 0);

  } catch (error) {
    console.error(JSON.stringify({
      valid: false,
      errors: [{ message: error.message }],
      warnings: []
    }));
    process.exit(2);
  } finally {
    // Cleanup
    if (fs.existsSync(tmpFile)) {
      fs.unlinkSync(tmpFile);
    }
  }
}

// Read from stdin
let code = '';
process.stdin.setEncoding('utf8');
process.stdin.on('data', chunk => { code += chunk; });
process.stdin.on('end', () => validateTypeScript(code));
```

**Tests**:
- Test valid TypeScript code
- Test invalid TypeScript code (type errors)
- Test syntax errors
- Test ESLint validation
- Test timeout handling
- Test cleanup

---

### **Task 9: Update Tests**
**Priority**: P1
**Estimated Time**: 3-4 days

**Update/Create**:
- `backend/tests/generation/test_generator_service.py` (major rewrite)
- `backend/tests/generation/test_llm_generator.py` (new)
- `backend/tests/generation/test_code_validator.py` (new)
- `backend/tests/generation/test_prompt_builder.py` (new)

**Acceptance Criteria**:

**test_generator_service.py** (rewrite):
- [ ] Test new simplified pipeline:
  ```python
  async def test_generate_success_path():
      """Test successful generation with valid code."""
      result = await generator.generate(request)
      assert result.success
      assert result.component_code
      assert result.stories_code
      assert result.metadata.validation_attempts == 0

  async def test_generate_with_validation_fixes():
      """Test generation that needs validation fixes."""
      # Mock LLM to return invalid code first, then valid
      result = await generator.generate(request)
      assert result.success
      assert result.metadata.validation_attempts == 1

  async def test_generate_validation_failure():
      """Test generation that fails validation after retries."""
      result = await generator.generate(request)
      assert not result.success
      assert result.validation_errors
      assert result.metadata.validation_attempts == 2
  ```
- [ ] Test LLM generation with mocks (use `unittest.mock`)
- [ ] Test validation loop convergence
- [ ] Test fix retries (0, 1, 2 attempts)
- [ ] Test error handling (LLM errors, validation errors)
- [ ] Test performance metrics
- [ ] Test LangSmith tracing (mock tracer)

**test_llm_generator.py** (new):
- [ ] Test prompt building
- [ ] Test structured output parsing
- [ ] Test error handling (API errors, malformed responses)
- [ ] Test token usage tracking
- [ ] Test retry logic
- [ ] Mock OpenAI API calls

**test_code_validator.py** (new):
- [ ] Test TypeScript validation:
  - Valid code passes
  - Invalid code detected
  - Syntax errors handled
- [ ] Test ESLint validation:
  - Valid code passes
  - Lint errors detected
- [ ] Test fix loop logic:
  - Converges after 1 fix
  - Converges after 2 fixes
  - Fails after max retries
- [ ] Test quality scoring
- [ ] Test parallel validation
- [ ] Mock Node.js subprocess calls

**test_prompt_builder.py** (new):
- [ ] Test prompt construction:
  - Pattern formatting
  - Token formatting
  - Requirements formatting
- [ ] Test exemplar selection:
  - Correct type selection
  - Fallback logic
  - Token budget
- [ ] Test token counting
- [ ] Test prompt versioning

**Integration Tests**:
- [ ] End-to-end generation with real LLM (marked as `@pytest.mark.integration`)
- [ ] Validation success cases
- [ ] Validation failure + fix cases
- [ ] Performance benchmarks
- [ ] Quality metrics

**Test Coverage**:
- [ ] Maintain overall coverage >90%
- [ ] New modules: >95% coverage
- [ ] Integration tests for critical paths

**Estimated Time**: 3-4 days

---

### **Task 10: Update API Endpoints**
**Priority**: P1
**Estimated Time**: 1 day

**Update**: `backend/src/api/v1/routes/generation.py`

**Acceptance Criteria**:
- [ ] Update response schema to include:
  ```python
  class GenerationResponse(BaseModel):
      success: bool
      component_code: Optional[str]
      stories_code: Optional[str]
      files: Dict[str, str]
      metadata: GenerationMetadata
      validation_results: Optional[ValidationResults]  # New
      quality_scores: Optional[QualityScores]  # New
      error: Optional[str]

  class ValidationResults(BaseModel):
      attempts: int
      final_status: str  # "passed", "failed", "skipped"
      typescript_errors: List[ValidationError]
      eslint_errors: List[ValidationError]

  class QualityScores(BaseModel):
      compilation: bool
      linting: int  # 0-100
      type_safety: int  # 0-100
      overall: int  # 0-100
  ```
- [ ] Add streaming support (optional):
  - Stream generation progress (`GENERATING`, `VALIDATING`, `POST_PROCESSING`)
  - Stream validation status
  - Use Server-Sent Events (SSE)
- [ ] Error handling improvements:
  - LLM API failures (clear error messages)
  - Validation failures (show errors)
  - Timeout errors (graceful degradation)
  - Rate limiting (429 errors)
- [ ] Update OpenAPI docs:
  - New response schema
  - Validation results
  - Quality scores
  - Error codes
- [ ] Add feature flag support:
  - `ENABLE_LLM_GENERATION` env var
  - Fall back to old pipeline if disabled
  - A/B testing support

**Tests**:
- Test successful generation response
- Test validation failure response
- Test error handling
- Test streaming (if implemented)
- Test OpenAPI schema validation

---

### **Task 11: Frontend Updates**
**Priority**: P2 (Can wait until backend complete)
**Estimated Time**: 2 days

**Update**:
- `app/src/app/preview/page.tsx`
- `app/src/components/composite/GenerationProgress.tsx`
- `app/src/types/generation.types.ts`

**Acceptance Criteria**:
- [ ] Update progress stages (3 stages now):
  - ✅ Generating (LLM generation)
  - ✅ Validating (TypeScript + ESLint)
  - ✅ Post-Processing (Formatting)
- [ ] Show validation results in UI:
  - ✅ TypeScript compilation status
  - ✅ ESLint validation status
  - ⚠️  Warnings count
  - 🔧 Fix attempts (if any)
- [ ] Show quality scores:
  - Overall quality score: 8.5/10
  - Type safety: 95/100
  - Linting: 90/100
  - Accessibility: 100/100
- [ ] Show fix attempts indicator:
  - "✓ Generated perfectly on first try"
  - "🔧 Fixed 1 issue automatically"
  - "🔧 Fixed 2 issues after validation"
- [ ] Error messages more detailed:
  - Show validation errors with line numbers
  - Suggest fixes or regeneration
- [ ] Quality indicators on Quality tab:
  ```tsx
  <div className="space-y-4">
    <QualityMetric
      label="TypeScript Compilation"
      status={metadata.validation.typescript_passed ? "pass" : "fail"}
      details={metadata.validation.typescript_errors}
    />
    <QualityMetric
      label="ESLint Validation"
      status={metadata.validation.eslint_passed ? "pass" : "fail"}
      details={`${metadata.validation.eslint_warnings} warnings`}
    />
    <QualityScore
      label="Overall Quality"
      score={metadata.quality_scores.overall}
      breakdown={{
        "Type Safety": metadata.quality_scores.type_safety,
        "Code Quality": metadata.quality_scores.linting,
        "Accessibility": metadata.quality_scores.accessibility
      }}
    />
  </div>
  ```
- [ ] Update types to match new API response

**Tests**:
- Test UI rendering with validation results
- Test quality score display
- Test error message display
- Test fix attempts indicator

---

### **Task 12: Cleanup - Delete Old Modules** 🗑️
**Priority**: P2 (After Tasks 1-7 complete)
**Estimated Time**: 1 day

**Delete Files** (12 files total):

**Backend Modules** (6 files):
```bash
❌ backend/src/generation/token_injector.py
❌ backend/src/generation/tailwind_generator.py
❌ backend/src/generation/requirement_implementer.py
❌ backend/src/generation/a11y_enhancer.py
❌ backend/src/generation/type_generator.py
❌ backend/src/generation/storybook_generator.py
```

**Test Files** (6 files):
```bash
❌ backend/tests/generation/test_token_injector.py
❌ backend/tests/generation/test_tailwind_generator.py
❌ backend/tests/generation/test_requirement_implementer.py
❌ backend/tests/generation/test_a11y_enhancer.py
❌ backend/tests/generation/test_type_generator.py
❌ backend/tests/generation/test_storybook_generator.py
```

**Keep Files** (4 files + 2 tests):
```bash
✅ backend/src/generation/provenance.py          # Traceability
✅ backend/src/generation/import_resolver.py     # Import cleanup
✅ backend/src/generation/types.py               # Type definitions (update)
✅ backend/src/generation/README.md              # Documentation (update)
✅ backend/tests/generation/test_provenance.py
✅ backend/tests/generation/test_import_resolver.py
```

**Acceptance Criteria**:
- [ ] Delete all 12 files listed above
- [ ] Remove imports from `generator_service.py`
- [ ] Update `types.py` with new schemas
- [ ] Update README.md with new architecture
- [ ] Verify all tests still pass
- [ ] Check for any remaining references (use `grep`)
- [ ] Update `.gitignore` if needed

**Commands**:
```bash
# Delete old modules
rm backend/src/generation/{token_injector,tailwind_generator,requirement_implementer,a11y_enhancer,type_generator,storybook_generator}.py

# Delete old tests
rm backend/tests/generation/test_{token_injector,tailwind_generator,requirement_implementer,a11y_enhancer,type_generator,storybook_generator}.py

# Check for references
grep -r "token_injector\|tailwind_generator\|requirement_implementer" backend/

# Run tests
pytest backend/tests/generation/ -v
```

---

### **Task 13: Update Documentation**
**Priority**: P2
**Estimated Time**: 1-2 days

**Update Files**:
- `backend/src/generation/README.md`
- `.claude/epics/04-code-generation.md`
- `CLAUDE.md` (if needed)

**Create Files**:
- `backend/src/generation/PROMPTING_GUIDE.md` (new)
- `backend/src/generation/TROUBLESHOOTING.md` (new)

**Acceptance Criteria**:

**README.md**:
- [ ] Update architecture diagram (8 stages → 3 stages)
- [ ] Document LLM-first approach
- [ ] Update module list (remove old, add new)
- [ ] Update performance targets
- [ ] Update usage examples
- [ ] Add troubleshooting section

**04-code-generation.md**:
- [ ] Mark Epic 4 as complete
- [ ] Reference Epic 4.5 for improvements
- [ ] Document migration path
- [ ] Update success metrics

**PROMPTING_GUIDE.md** (new):
- [ ] System prompt template
- [ ] User prompt structure
- [ ] Exemplar format
- [ ] Few-shot learning strategy
- [ ] Token optimization tips
- [ ] Prompt versioning guide
- [ ] Testing prompts
- [ ] A/B testing strategy

**TROUBLESHOOTING.md** (new):
- [ ] Common issues and solutions:
  - LLM returns invalid code
  - Validation fails repeatedly
  - Slow generation time
  - High LLM costs
- [ ] Debugging with LangSmith
- [ ] Log analysis
- [ ] Performance profiling
- [ ] Quality debugging

---

### **Task 14: Performance Optimization**
**Priority**: P2 (After core functionality works)
**Estimated Time**: 2-3 days

**Acceptance Criteria**:
- [ ] **Parallel validation** (TypeScript + ESLint):
  - Run both validations simultaneously
  - Aggregate results
  - Target: 3-5s validation time
- [ ] **Prompt caching**:
  - Cache system prompt (static)
  - Cache exemplars (rarely change)
  - OpenAI prompt caching API
- [ ] **Lazy load exemplars**:
  - Load on first use
  - Keep in memory after loading
  - Invalidate cache on updates
- [ ] **Optimize LLM parameters**:
  - Temperature: 0.3-0.5 (balanced)
  - Max tokens: Optimized per call
  - Model: gpt-4o (fast) vs gpt-4 (quality)
  - Test different models for cost/quality tradeoff
- [ ] **Request caching**:
  - Cache identical requests (same pattern + tokens + requirements)
  - TTL: 1 hour
  - Redis or in-memory cache
- [ ] **Target latency**:
  - p50: <30s (from ~60s) ✅
  - p95: <60s (from ~90s) ✅
  - p99: <90s ✅
- [ ] **Token usage optimization**:
  - Compress requirements representation
  - Truncate long patterns if needed
  - Remove redundant information
- [ ] **Monitoring**:
  - Track latency percentiles
  - Track LLM costs per request
  - Track cache hit rates
  - Alert on regressions

**Tests**:
- Performance benchmarks
- Load testing (concurrent requests)
- Cache hit rate testing

---

### **Task 15: Quality Monitoring Dashboard**
**Priority**: P3 (Nice to have)
**Estimated Time**: 2 days

**Create**: `backend/src/monitoring/generation_metrics.py`

**Acceptance Criteria**:
- [ ] Track quality metrics:
  - Generation success rate (%)
  - Validation pass rate (%)
  - Fix success rate by attempt (1st, 2nd)
  - Average quality scores (compilation, linting, type safety)
  - Latency percentiles (p50, p95, p99)
  - LLM token usage and costs
- [ ] LangSmith integration:
  - Custom metadata in traces
  - Quality scores in trace metadata
  - Search/filter by quality
- [ ] Prometheus metrics export:
  ```python
  generation_success_rate = Gauge(
      'generation_success_rate',
      'Percentage of successful generations'
  )

  validation_pass_rate = Gauge(
      'validation_pass_rate',
      'Percentage of validations that pass'
  )

  generation_latency = Histogram(
      'generation_latency_seconds',
      'Generation latency distribution'
  )
  ```
- [ ] Alert thresholds:
  - Success rate <90% → Warning
  - Success rate <80% → Critical
  - p95 latency >60s → Warning
  - Fix rate >30% → Warning (too many fixes needed)
- [ ] Weekly quality reports:
  - Email summary
  - Quality trends
  - Top errors/issues
  - Cost analysis
- [ ] Dashboard (optional):
  - Grafana dashboard JSON
  - Real-time metrics
  - Quality trends over time

---

## File Changes Summary

### 📊 Statistics
- **New Files**: 7
- **Updated Files**: 6
- **Deleted Files**: 12
- **Net Change**: -5 files (19 → 14)
- **LOC Reduction**: ~50% (~2800 → ~1500)

### ✨ New Files (7)
```
backend/src/generation/llm_generator.py           (~300 LOC)
backend/src/generation/code_validator.py          (~400 LOC)
backend/src/generation/prompt_builder.py          (~250 LOC)
backend/src/generation/exemplar_loader.py         (~150 LOC)
backend/scripts/validate_typescript.js            (~150 LOC)
backend/scripts/validate_eslint.js                (~100 LOC)
backend/tests/generation/test_llm_generator.py    (~200 LOC)
backend/tests/generation/test_code_validator.py   (~200 LOC)
backend/tests/generation/test_prompt_builder.py   (~150 LOC)
```

### 📝 Updated Files (6)
```
backend/src/generation/generator_service.py       (major refactor)
backend/src/generation/pattern_parser.py          (simplify)
backend/src/generation/code_assembler.py          (simplify)
backend/src/generation/types.py                   (new schemas)
backend/src/api/v1/routes/generation.py           (update response)
backend/tests/generation/test_generator_service.py (rewrite)
```

### ❌ Deleted Files (12)
```
backend/src/generation/token_injector.py          (~250 LOC)
backend/src/generation/tailwind_generator.py      (~200 LOC)
backend/src/generation/requirement_implementer.py (~310 LOC)
backend/src/generation/a11y_enhancer.py           (~348 LOC)
backend/src/generation/type_generator.py          (~315 LOC)
backend/src/generation/storybook_generator.py     (~250 LOC)
backend/tests/generation/test_token_injector.py
backend/tests/generation/test_tailwind_generator.py
backend/tests/generation/test_requirement_implementer.py
backend/tests/generation/test_a11y_enhancer.py
backend/tests/generation/test_type_generator.py
backend/tests/generation/test_storybook_generator.py
```

### ✅ Kept Files (6)
```
backend/src/generation/provenance.py              (no changes)
backend/src/generation/import_resolver.py         (no changes)
backend/src/generation/types.py                   (update only)
backend/src/generation/README.md                  (update docs)
backend/tests/generation/test_provenance.py       (no changes)
backend/tests/generation/test_import_resolver.py  (no changes)
```

---

## Dependencies

### Existing Dependencies (Already have)
```python
# requirements.txt
openai>=1.0.0          # Already present
langchain>=0.1.0       # Already present
langsmith>=0.1.0       # Already present
fastapi>=0.100.0       # Already present
pydantic>=2.0.0        # Already present
```

### New Dependencies (Add to requirements.txt)
```python
tiktoken>=0.5.0        # Token counting for prompt optimization
```

### Node.js Dependencies (Already in app/package.json)
```json
{
  "typescript": "5.9.3",
  "eslint": "^9",
  "@typescript-eslint/parser": "^8"
}
```

---

## Success Metrics

### Quality Metrics
| Metric | Before | Target | Measurement |
|--------|--------|--------|-------------|
| **TypeScript Compilation Success** | ~60% | 95%+ | Validation pass rate |
| **Code Quality Score** | 5/10 | 8-9/10 | Composite quality metric |
| **User Regeneration Rate** | High | <10% | Usage analytics |
| **Zero Type Errors** | 60% | 95%+ | Validation results |
| **Zero Lint Errors** | 70% | 95%+ | Validation results |

### Performance Metrics
| Metric | Before | Target | Measurement |
|--------|--------|--------|-------------|
| **Generation Latency (p50)** | ~60s | <30s | LangSmith traces |
| **Generation Latency (p95)** | ~90s | <60s | LangSmith traces |
| **Generation Latency (p99)** | ~120s | <90s | LangSmith traces |
| **Validation Latency** | N/A | <5s | Validation timing |
| **First-Try Success Rate** | ~60% | 80%+ | Validation attempts |

### Codebase Metrics
| Metric | Before | Target | Measurement |
|--------|--------|--------|-------------|
| **Lines of Code (generation module)** | ~2800 | ~1500 | Code analysis |
| **Number of Modules** | 11 | 7 | File count |
| **Test Coverage** | ~85% | >90% | pytest-cov |
| **Cyclomatic Complexity** | High | Medium | Code analysis |

---

## Rollout Strategy

### Phase 1: Core Implementation (Week 1-2)
**Tasks**: 1, 2, 3, 5, 8

**Goals**:
- [ ] LLM generator working
- [ ] Validation loop working
- [ ] Simplified generator service
- [ ] Feature flag: `ENABLE_LLM_GENERATION=false` (testing only)

**Testing**:
- Unit tests for new modules
- Integration test with Button pattern
- Compare quality vs old pipeline

**Success Criteria**:
- Button component generates successfully
- Validation catches errors and fixes them
- Quality score >7/10

---

### Phase 2: Quality & Testing (Week 2-3)
**Tasks**: 4, 6, 7, 9

**Goals**:
- [ ] Exemplar library complete (5 exemplars)
- [ ] Pattern parser simplified
- [ ] Code assembler simplified
- [ ] All tests passing

**Testing**:
- Test with all pattern types (Button, Card, Input, etc.)
- A/B testing: old vs new pipeline
- Performance benchmarking

**Success Criteria**:
- All patterns generate successfully
- Quality score >8/10 on average
- Test coverage >90%

---

### Phase 3: Cleanup & Production (Week 3-4)
**Tasks**: 10, 11, 12, 13

**Goals**:
- [ ] API updated with new response schema
- [ ] Frontend displays validation results
- [ ] Old modules deleted
- [ ] Documentation updated

**Testing**:
- End-to-end testing
- User acceptance testing
- Performance testing under load

**Success Criteria**:
- Production deployment successful
- No regressions
- User feedback positive

---

### Phase 4: Optimization (Week 4+)
**Tasks**: 14, 15

**Goals**:
- [ ] Performance optimized (p50 <30s)
- [ ] Monitoring dashboard live
- [ ] Cost optimization

**Testing**:
- Load testing
- Cost analysis
- Quality trending

**Success Criteria**:
- Latency targets met
- Quality sustained >8/10
- Costs acceptable

---

## Risks & Mitigation

| Risk | Impact | Probability | Mitigation |
|------|--------|-------------|------------|
| **LLM generates invalid code** | High | Medium | Validation loop with max 2 fixes, detailed error reporting |
| **LLM latency too high** | Medium | Low | Optimize prompts, parallel validation, caching |
| **LLM costs too high** | Medium | Medium | Prompt optimization, caching, model selection (gpt-4o) |
| **Quality regression** | High | Low | A/B testing, gradual rollout, feature flag |
| **Breaking existing users** | High | Low | Feature flag, backward compatibility during transition |
| **Validation scripts fail** | High | Low | Graceful degradation, skip validation if unavailable |
| **Fix loop doesn't converge** | Medium | Medium | Max retry limit (2), return detailed errors |
| **Team resistance to LLM approach** | Medium | Low | Show quality improvements, demo working system |

---

## Definition of Done

- [ ] All 15 tasks completed with acceptance criteria met
- [ ] LLM generator creates complete, coherent components
- [ ] Validation loop catches and fixes errors (80%+ success)
- [ ] TypeScript compilation success ≥95%
- [ ] Code quality score ≥8/10
- [ ] Generation latency: p50 <30s, p95 <60s
- [ ] All 12 old modules deleted
- [ ] Test coverage >90%
- [ ] Documentation updated
- [ ] Production deployment successful
- [ ] No critical bugs
- [ ] User feedback positive
- [ ] Monitoring dashboard showing quality metrics

---

## Dependencies & Blockers

### Depends On
- ✅ Epic 4 (Current Code Generation) - Complete
- ✅ OpenAI API access - Have
- ✅ LangSmith setup - Have
- ✅ Node.js installed - Have

### Blocks
- Epic 5 (Quality Validation) - Can leverage validation loop
- Epic 8 (Regeneration) - Will use same LLM pipeline

---

## Related Epics

- **Epic 4**: Current code generation (completed, being replaced)
- **Epic 5**: Quality validation (can leverage validation loop from Task 2)
- **Epic 8**: Regeneration & versioning (will use same LLM pipeline)

---

## References

- [Epic 4 Specification](./04-code-generation.md)
- [Epic 5 Specification](./05-quality-validation.md)
- [OpenAI Structured Outputs](https://platform.openai.com/docs/guides/structured-outputs)
- [LangSmith Tracing](https://docs.smith.langchain.com/)
- [TypeScript Compiler API](https://github.com/microsoft/TypeScript/wiki/Using-the-Compiler-API)
- [ESLint Node.js API](https://eslint.org/docs/latest/integrate/nodejs-api)

---

## Notes

### Why LLM-First?

**Current approach problems**:
- 8 disconnected stages → fragmented output
- Regex manipulation → brittle, error-prone
- No validation → ships broken code
- Hard to maintain → 2800 LOC of complex logic

**LLM-first benefits**:
- Single coherent generation → better quality
- Semantic understanding → natural integration
- Self-healing via validation → guaranteed working code
- Simpler architecture → easier to maintain and improve

### Cost Considerations

**Estimated costs per component**:
- Generation: ~8000 tokens input, ~2000 tokens output
- Cost: ~$0.10-0.15 per component (gpt-4o)
- With caching: ~$0.05-0.08 per component
- Fix attempts: +$0.05 per fix

**Mitigation**:
- Use gpt-4o (cheaper, faster than gpt-4)
- Implement prompt caching
- Cache identical requests
- Optimize prompt length

### Future Enhancements

- Fine-tuned model for component generation
- Multi-framework support (Vue, Angular, Svelte)
- Visual preview generation
- Component testing generation
- Performance optimization suggestions
- Automatic documentation generation
