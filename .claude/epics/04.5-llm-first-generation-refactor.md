# Epic 4.5: LLM-First Code Generation Refactor

**Status**: Not Started
**Priority**: Critical
**Epic Owner**: Backend/AI Team
**Estimated Tasks**: 15
**Depends On**: Epic 4 (Current Code Generation)
**Timeline**: 4-5 weeks

---

## Overview

Replace the current fragmented 8-stage template-based pipeline with a simplified 3-stage LLM-first architecture that generates coherent, high-quality component code through intelligent generation and validation.

**Current Problem**: The existing pipeline uses regex-based string manipulation across 8 disconnected stages, resulting in:
- Fragmented, incoherent code with duplicate types
- Low TypeScript compilation success rate (~60%)
- Poor integration of requirements, tokens, and accessibility
- Complex, hard-to-maintain codebase (~2800 LOC)
- Brittle regex patterns that break easily

**Solution**: Simplify to 3-stage LLM-first pipeline:
1. **Generate** - Single LLM call creates complete, coherent component
2. **Validate** - TypeScript/ESLint validation with LLM-based fixes
3. **Post-Process** - Import resolution, formatting, provenance

---

## Goals

1. **Improve code quality**: 60% ‚Üí 95%+ TypeScript compilation success
2. **Simplify architecture**: 8 stages ‚Üí 3 stages
3. **Reduce complexity**: Delete 5 overengineered modules, add 3 focused modules
4. **Enable self-healing**: Validation loop with LLM-based error correction
5. **Maintain traceability**: Keep provenance tracking and LangSmith observability
6. **Reduce codebase**: ~2800 LOC ‚Üí ~1500 LOC
7. **Improve latency**: p50 60s ‚Üí 30s, p95 90s ‚Üí 60s

---

## Success Criteria

- ‚úÖ TypeScript compilation success rate ‚â•95%
- ‚úÖ Code quality score 8-9/10 (from 5/10)
- ‚úÖ Generation latency p50 <30s (from ~60s)
- ‚úÖ Generation latency p95 <60s (from ~90s)
- ‚úÖ User regeneration rate <10% (from high)
- ‚úÖ Test coverage >90%
- ‚úÖ Codebase reduced by ~50%
- ‚úÖ All tests passing
- ‚úÖ Production deployment successful

---

## Architecture Comparison

### Before (Current - 8 Stages)
```
Pattern JSON ‚Üí Parse Pattern
              ‚Üì
          Token Injection (regex)
              ‚Üì
          Tailwind Generation (templates)
              ‚Üì
          Requirement Implementation (regex)
              ‚Üì
          A11y Enhancement (regex)
              ‚Üì
          Type Generation (regex)
              ‚Üì
          Storybook Generation (templates)
              ‚Üì
          Code Assembly (concatenation)
              ‚Üì
          Prettier Formatting
              ‚Üì
          Output (often broken)
```

**Problems**:
- Each stage operates independently without context
- Stages conflict (duplicate types, conflicting ARIA)
- Regex manipulation is fragile and error-prone
- No validation before returning to user
- Code fragments don't integrate well

### After (LLM-First - 3 Stages)
```
Pattern JSON (reference) + Tokens + Requirements
              ‚Üì
          LLM Generation (single pass)
          - Complete component code
          - Integrated types, props, events
          - Built-in accessibility
          - Storybook stories
              ‚Üì
          Validation Loop
          - TypeScript compilation check
          - ESLint validation
          - If errors: LLM fixes (max 2 retries)
              ‚Üì
          Post-Processing
          - Import resolution & ordering
          - Provenance header injection
          - Prettier formatting
              ‚Üì
          Output (validated, working code)
```

**Benefits**:
- Single coherent generation with full context
- Semantic understanding of requirements
- Self-healing through validation loop
- Guaranteed valid output
- Much simpler to maintain and improve

---

## Tasks

### **Task 1: Build LLM Code Generator (Core)** ‚≠ê
**Priority**: P0 (Critical Path)
**Estimated Time**: 3-4 days

**Create**: `backend/src/generation/llm_generator.py`

**Acceptance Criteria**:
- [ ] `LLMComponentGenerator` class with structured output
- [ ] Comprehensive prompt builder that includes:
  - Pattern code as reference example
  - Design tokens with semantic descriptions
  - All requirements (props, events, states, a11y)
  - Component naming and conventions
  - TypeScript strict mode requirements
  - Accessibility requirements
  - shadcn/ui style guidelines
- [ ] Use OpenAI structured outputs or JSON mode
- [ ] Generate complete component code (not fragments)
- [ ] Generate Storybook stories in same call
- [ ] LangSmith tracing for all LLM calls
- [ ] Error handling with retries (3 attempts)
- [ ] Response validation (check for required fields)
- [ ] Token usage tracking and optimization

**Output Schema**:
```python
@dataclass
class LLMGeneratedCode:
    component_code: str      # Complete .tsx file content
    stories_code: str        # Complete .stories.tsx content
    imports: List[str]       # List of import statements
    exports: List[str]       # List of exported names
    explanation: str         # Why code was generated this way (for debugging)
    token_usage: Dict        # Prompt/completion tokens used
```

**Prompt Structure**:
```
System: You are an expert React/TypeScript developer...

User:
## Task
Generate a production-ready React component based on the requirements below.

## Reference Pattern (shadcn/ui)
[Pattern code as example]

## Design Tokens
Colors: {...}
Typography: {...}
Spacing: {...}

## Requirements
Props: [{name, type, description}]
Events: [{name, signature}]
States: [{name, type, default}]
Accessibility: [{requirement}]

## Constraints
- TypeScript strict mode (no 'any' types)
- Include proper ARIA attributes
- Use design tokens via CSS variables
- Follow shadcn/ui conventions
- Export component with displayName

## Output Format
Return JSON with: component_code, stories_code, imports, exports, explanation
```

**Tests**:
- Test prompt construction
- Test structured output parsing
- Test error handling
- Test token usage tracking
- Test with various component types

**Dependencies**: OpenAI API, LangSmith

---

### **Task 2: Build Code Validator with Fix Loop** ‚≠ê
**Priority**: P0 (Critical Path)
**Estimated Time**: 3-4 days

**Create**: `backend/src/generation/code_validator.py`

**Acceptance Criteria**:
- [ ] TypeScript validation using `ts.createProgram` API (via Node.js subprocess)
- [ ] ESLint validation via Node.js subprocess
- [ ] Validation result parsing (errors, warnings, line numbers)
- [ ] LLM-based fix generator that:
  - Takes original code + validation errors
  - Generates fixed code with explanation
  - Preserves working parts
  - Only fixes specific issues
- [ ] Iterative fix loop (max 2-3 attempts)
- [ ] Quality scoring:
  - Compilation success: boolean
  - Lint error count: int
  - Lint warning count: int
  - Type coverage: percentage
- [ ] Performance tracking (<5s validation time)
- [ ] Graceful degradation if validation unavailable
- [ ] Parallel validation (TypeScript + ESLint simultaneously)

**Implementation**:
```python
class CodeValidator:
    async def validate_and_fix(
        self,
        code: str,
        max_retries: int = 2
    ) -> ValidationResult:
        """Validate code, use LLM to fix if needed."""

        for attempt in range(max_retries + 1):
            # Run validations in parallel
            ts_result, lint_result = await asyncio.gather(
                self._validate_typescript(code),
                self._validate_eslint(code)
            )

            if ts_result.valid and lint_result.valid:
                return ValidationResult(
                    valid=True,
                    code=code,
                    attempts=attempt,
                    quality_score=self._calculate_quality(ts_result, lint_result)
                )

            if attempt < max_retries:
                # Use LLM to fix errors
                code = await self._llm_fix_errors(
                    code,
                    ts_result.errors,
                    lint_result.errors
                )

        return ValidationResult(
            valid=False,
            code=code,
            attempts=max_retries,
            errors=[...ts_result.errors, ...lint_result.errors]
        )
```

**Fix Prompt Strategy**:
```
You are debugging TypeScript/ESLint errors in React component code.

## Original Code
[code]

## Validation Errors
TypeScript: [errors with line numbers]
ESLint: [errors with line numbers]

## Task
Fix ONLY the specific errors listed above. Preserve all working code.
Return the complete fixed code.
```

**Tests**:
- Test TypeScript validation (valid/invalid code)
- Test ESLint validation
- Test fix generation
- Test fix loop convergence
- Test quality scoring
- Test parallel validation performance

**Dependencies**: Node.js, TypeScript, ESLint, OpenAI API

---

### **Task 3: Build Prompt Builder** ‚≠ê
**Priority**: P0 (Critical Path)
**Estimated Time**: 2-3 days

**Create**: `backend/src/generation/prompt_builder.py`

**Acceptance Criteria**:
- [ ] `PromptBuilder` class with template management
- [ ] System prompt for component generation
- [ ] Dynamic prompt construction from:
  - Pattern reference (format as example)
  - Design tokens (explain semantic meaning)
  - Requirements (structured by category)
  - Component name and type
  - Exemplars (few-shot learning)
- [ ] Few-shot examples library:
  - Load 2-3 relevant exemplars based on component type
  - Include input (requirements) and output (code)
- [ ] Anti-pattern examples (what NOT to do):
  - Using `any` types
  - Missing ARIA attributes
  - Hardcoded colors instead of tokens
- [ ] Token usage optimization:
  - Truncate long patterns if needed
  - Compress requirements representation
  - Target: <8000 tokens per prompt
- [ ] Prompt versioning for A/B testing
- [ ] Validation constraints in prompt:
  - Zero `any` types
  - All props must have types
  - ARIA attributes required
  - Proper import structure
  - Component displayName required

**Implementation**:
```python
class PromptBuilder:
    def __init__(self, exemplar_loader: ExemplarLoader):
        self.exemplar_loader = exemplar_loader
        self.system_prompt = self._load_system_prompt()

    def build_generation_prompt(
        self,
        pattern: PatternStructure,
        tokens: Dict,
        requirements: Dict,
        component_name: str
    ) -> List[Message]:
        """Build complete prompt for generation."""

        # Select relevant exemplars
        exemplars = self.exemplar_loader.select_exemplars(
            component_type=pattern.component_type,
            num_examples=2
        )

        # Build user message
        user_message = f"""
## Task
Generate a production-ready {component_name} component.

## Reference Pattern (shadcn/ui)
{self._format_pattern(pattern)}

## Design Tokens
{self._format_tokens(tokens)}

## Requirements
{self._format_requirements(requirements)}

## Examples
{self._format_exemplars(exemplars)}

## Anti-Patterns (Avoid)
- Using 'any' types
- Missing ARIA attributes
- Hardcoded color values

## Output Format
Return JSON with: component_code, stories_code, imports, exports, explanation
"""

        return [
            {"role": "system", "content": self.system_prompt},
            {"role": "user", "content": user_message}
        ]
```

**Tests**:
- Test prompt construction
- Test token counting
- Test exemplar selection
- Test prompt versioning
- Test with various component types

---

### **Task 4: Create Exemplar Library**
**Priority**: P1 (Required for quality)
**Estimated Time**: 2-3 days

**Create**:
- `backend/data/exemplars/` directory
- `backend/src/generation/exemplar_loader.py`

**Acceptance Criteria**:
- [ ] 5 hand-crafted exemplar components:
  - **Button**: variants, sizes, accessibility, icons
  - **Card**: composition, semantic HTML, proper structure
  - **Input**: validation, error states, labels, hints
  - **Checkbox**: controlled, accessibility, labels
  - **Alert**: ARIA live regions, icons, variants
- [ ] Each exemplar includes:
  - `input.json`: Requirements and tokens
  - `output.tsx`: Perfect generated component
  - `output.stories.tsx`: Storybook stories
  - `metadata.json`: Why this is a good example
  - `comments.md`: Explain key decisions
- [ ] JSON metadata for each exemplar:
  ```json
  {
    "component_type": "button",
    "demonstrates": [
      "variant prop with union types",
      "proper ARIA attributes",
      "forwardRef pattern",
      "displayName usage",
      "CVA for variants"
    ],
    "tokens_used": ["colors", "spacing", "typography"],
    "quality_score": 95
  }
  ```
- [ ] `ExemplarLoader` class:
  - Load exemplars from disk
  - Select relevant exemplars by component type
  - Format for prompt inclusion
  - Cache loaded exemplars
- [ ] Selection logic:
  - Prefer same component type (Button ‚Üí Button)
  - Fall back to similar types (Checkbox ‚Üí Switch)
  - Limit to 2-3 examples per prompt (token budget)

**Exemplar Structure**:
```
backend/data/exemplars/
‚îú‚îÄ‚îÄ button/
‚îÇ   ‚îú‚îÄ‚îÄ input.json
‚îÇ   ‚îú‚îÄ‚îÄ output.tsx
‚îÇ   ‚îú‚îÄ‚îÄ output.stories.tsx
‚îÇ   ‚îú‚îÄ‚îÄ metadata.json
‚îÇ   ‚îî‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ card/
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îî‚îÄ‚îÄ input/
    ‚îî‚îÄ‚îÄ ...
```

**Tests**:
- Test exemplar loading
- Test selection logic
- Test formatting for prompts
- Test caching

---

### **Task 5: Refactor Generator Service** ‚≠ê
**Priority**: P0 (Critical Path)
**Estimated Time**: 2-3 days

**Update**: `backend/src/generation/generator_service.py`

**Acceptance Criteria**:
- [ ] Remove all old stage calls:
  - ‚ùå `_inject_tokens()`
  - ‚ùå `_generate_tailwind()`
  - ‚ùå `_implement_requirements()`
  - ‚ùå `_enhance_accessibility()`
  - ‚ùå `_generate_types()`
  - ‚ùå `_generate_storybook_stories()`
- [ ] New simplified pipeline:
  1. **Load pattern** (as reference only)
  2. **Build generation prompt** (with exemplars)
  3. **Call LLM generator** (single pass)
  4. **Validate generated code** (TypeScript + ESLint)
  5. **If invalid**: LLM fix loop (max 2 retries)
  6. **Post-process**: imports, provenance, formatting
  7. **Return result** with metadata
- [ ] Stage tracking updated to 3 stages:
  - `GENERATING`: LLM generation (~15-20s)
  - `VALIDATING`: TypeScript/ESLint validation (~3-5s)
  - `POST_PROCESSING`: Final assembly (~2-3s)
- [ ] Preserve LangSmith tracing:
  - Trace LLM calls
  - Trace validation
  - Trace fix attempts
- [ ] Detailed error reporting:
  - LLM generation errors
  - Validation errors (with line numbers)
  - Fix attempt history
- [ ] Performance metrics:
  - LLM latency (prompt + completion)
  - Validation latency
  - Fix loop latency
  - Total latency (target: <30s p50, <60s p95)
- [ ] Quality metrics in metadata:
  - Validation attempts (0 = perfect first try)
  - Final validation status (pass/fail)
  - Quality scores (compilation, linting, type safety)
  - Token usage

**Implementation**:
```python
class GeneratorService:
    @traceable(run_type="chain", name="generate_component_llm_first")
    async def generate(self, request: GenerationRequest) -> GenerationResult:
        """Generate component using LLM-first approach."""
        start_time = time.time()

        try:
            # Stage 1: Load pattern as reference
            pattern = await self._load_pattern(request.pattern_id)

            # Stage 2: Build prompt with exemplars
            prompt = self.prompt_builder.build_generation_prompt(
                pattern=pattern,
                tokens=request.tokens,
                requirements=request.requirements,
                component_name=request.component_name
            )

            # Stage 3: Generate with LLM
            self.current_stage = GenerationStage.GENERATING
            generated = await self.llm_generator.generate(prompt)

            # Stage 4: Validate and fix if needed
            self.current_stage = GenerationStage.VALIDATING
            validated = await self.code_validator.validate_and_fix(
                generated.component_code,
                max_retries=2
            )

            if not validated.valid:
                # Return with detailed errors
                return GenerationResult(
                    success=False,
                    error="Code validation failed after 2 fix attempts",
                    validation_errors=validated.errors,
                    metadata=self._build_metadata(start_time, generated, validated)
                )

            # Stage 5: Post-process
            self.current_stage = GenerationStage.POST_PROCESSING
            final_code = await self._post_process(
                validated.code,
                generated.stories_code,
                request.pattern_id,
                request.tokens,
                request.requirements
            )

            # Build successful result
            return GenerationResult(
                success=True,
                component_code=final_code["component"],
                stories_code=final_code["stories"],
                files=final_code["files"],
                metadata=self._build_metadata(start_time, generated, validated)
            )

        except Exception as e:
            logger.error(f"Generation failed: {e}")
            return GenerationResult(
                success=False,
                error=str(e),
                metadata=self._build_metadata(start_time)
            )
```

**Tests**:
- Test successful generation path
- Test validation failure path
- Test fix loop convergence
- Test error handling
- Test performance metrics
- Test LangSmith tracing

---

### **Task 6: Update Pattern Parser (Simplify)**
**Priority**: P1
**Estimated Time**: 1 day

**Update**: `backend/src/generation/pattern_parser.py`

**Acceptance Criteria**:
- [ ] Remove modification point detection (no longer needed):
  - ‚ùå `_find_modification_points()`
  - ‚ùå className location tracking
  - ‚ùå variant definition tracking
- [ ] Remove regex-based code analysis:
  - ‚ùå `_extract_props_interface()`
  - ‚ùå `_extract_imports()` (handled by ImportResolver)
- [ ] Keep only basic parsing:
  - ‚úÖ `load_pattern(pattern_id)` - Load JSON
  - ‚úÖ Extract metadata (name, type, variants)
  - ‚úÖ Return pattern code as string (reference only)
- [ ] Add pattern metadata extraction:
  - Component type (button, card, input, etc.)
  - Variants list (for prompt context)
  - Dependencies list
  - A11y features
- [ ] Return simplified `PatternStructure`:
  ```python
  @dataclass
  class PatternStructure:
      component_name: str
      component_type: str
      code: str  # Complete pattern code (reference)
      variants: List[str]
      dependencies: List[str]
      metadata: Dict[str, Any]
  ```
- [ ] Performance: <50ms per pattern
- [ ] Error handling for missing patterns

**Tests**:
- Test pattern loading
- Test metadata extraction
- Test error handling
- Test performance

---

### **Task 7: Simplify Code Assembler**
**Priority**: P1
**Estimated Time**: 1 day

**Update**: `backend/src/generation/code_assembler.py`

**Acceptance Criteria**:
- [ ] Remove multi-section assembly logic (no longer needed):
  - ‚ùå `_build_code_parts()` from multiple stages
  - ‚ùå Type definitions section
  - ‚ùå CSS variables section (now in code)
  - ‚ùå Component code section
- [ ] Keep only:
  - ‚úÖ **Provenance header injection** (delegate to ProvenanceGenerator)
  - ‚úÖ **Import resolution** (delegate to ImportResolver)
  - ‚úÖ **Prettier formatting** (via Node.js script)
- [ ] Simplified interface:
  ```python
  class CodeAssembler:
      async def assemble(
          self,
          component_code: str,  # Complete code from LLM
          stories_code: str,     # Complete stories from LLM
          pattern_id: str,
          tokens: Dict,
          requirements: Dict,
          component_name: str
      ) -> AssembledCode:
          """Assemble final code with header, imports, formatting."""

          # Add provenance header
          component_with_header = self.provenance_generator.inject_header(
              component_code,
              pattern_id,
              tokens,
              requirements
          )

          # Resolve and order imports
          component_with_imports = self.import_resolver.resolve_and_order(
              component_with_header
          )

          # Format with Prettier
          formatted_component = await self._format_code(component_with_imports)
          formatted_stories = await self._format_code(stories_code)

          return AssembledCode(
              component=formatted_component,
              stories=formatted_stories,
              files={
                  f"{component_name}.tsx": formatted_component,
                  f"{component_name}.stories.tsx": formatted_stories
              }
          )
  ```
- [ ] Input: complete component code string (not fragments)
- [ ] Output: formatted code with header and organized imports
- [ ] Performance: <2s for formatting

**Tests**:
- Test header injection
- Test import resolution
- Test formatting
- Test error handling

---

### **Task 8: Add Validation Scripts (Node.js)** ‚≠ê
**Priority**: P0 (Required for Task 2)
**Estimated Time**: 1-2 days

**Create**:
- `backend/scripts/validate_typescript.js`
- `backend/scripts/validate_eslint.js`

**Acceptance Criteria**:

**TypeScript Validation Script**:
- [ ] Accepts code via stdin
- [ ] Creates temp file in `.tmp/` directory
- [ ] Runs `ts.createProgram` with strict mode config
- [ ] Returns JSON with errors/warnings:
  ```json
  {
    "valid": false,
    "errors": [
      {
        "line": 42,
        "column": 10,
        "message": "Type 'string' is not assignable to type 'number'",
        "code": "TS2322"
      }
    ],
    "warnings": []
  }
  ```
- [ ] Cleanup temp files
- [ ] Exit codes: 0 (valid), 1 (errors), 2 (fatal)
- [ ] Performance: <2s for typical component

**ESLint Validation Script**:
- [ ] Accepts code via stdin
- [ ] Runs ESLint programmatically (no temp file needed)
- [ ] Use existing `app/eslint.config.mjs` configuration
- [ ] Returns JSON with errors/warnings:
  ```json
  {
    "valid": false,
    "errors": [
      {
        "line": 15,
        "column": 5,
        "message": "Missing return type on function",
        "ruleId": "@typescript-eslint/explicit-function-return-type"
      }
    ],
    "warnings": []
  }
  ```
- [ ] Exit codes: 0 (valid), 1 (errors), 2 (fatal)
- [ ] Performance: <2s for typical component

**Both scripts handle edge cases**:
- [ ] Malformed code (syntax errors)
- [ ] Missing dependencies (graceful degradation)
- [ ] Timeout after 10s
- [ ] Proper error messages

**Implementation** (`validate_typescript.js`):
```javascript
#!/usr/bin/env node

const ts = require('typescript');
const fs = require('fs');
const path = require('path');

async function validateTypeScript(code) {
  // Create temp file
  const tmpDir = path.join(__dirname, '../.tmp');
  if (!fs.existsSync(tmpDir)) {
    fs.mkdirSync(tmpDir, { recursive: true });
  }

  const tmpFile = path.join(tmpDir, `validate-${Date.now()}.tsx`);
  fs.writeFileSync(tmpFile, code);

  try {
    // Load tsconfig
    const configPath = path.join(__dirname, '../../app/tsconfig.json');
    const configFile = ts.readConfigFile(configPath, ts.sys.readFile);
    const parsedConfig = ts.parseJsonConfigFileContent(
      configFile.config,
      ts.sys,
      path.dirname(configPath)
    );

    // Create program and get diagnostics
    const program = ts.createProgram([tmpFile], parsedConfig.options);
    const diagnostics = ts.getPreEmitDiagnostics(program);

    // Format results
    const errors = [];
    const warnings = [];

    for (const diagnostic of diagnostics) {
      const message = ts.flattenDiagnosticMessageText(diagnostic.messageText, '\n');
      let line = 0, column = 0;

      if (diagnostic.file && diagnostic.start !== undefined) {
        const { line: lineNum, character } = diagnostic.file.getLineAndCharacterOfPosition(diagnostic.start);
        line = lineNum + 1;
        column = character + 1;
      }

      const issue = {
        line,
        column,
        message,
        code: `TS${diagnostic.code}`
      };

      if (diagnostic.category === ts.DiagnosticCategory.Error) {
        errors.push(issue);
      } else if (diagnostic.category === ts.DiagnosticCategory.Warning) {
        warnings.push(issue);
      }
    }

    // Output JSON
    const result = {
      valid: errors.length === 0,
      errors,
      warnings
    };

    console.log(JSON.stringify(result, null, 2));

    // Exit code
    process.exit(errors.length > 0 ? 1 : 0);

  } catch (error) {
    console.error(JSON.stringify({
      valid: false,
      errors: [{ message: error.message }],
      warnings: []
    }));
    process.exit(2);
  } finally {
    // Cleanup
    if (fs.existsSync(tmpFile)) {
      fs.unlinkSync(tmpFile);
    }
  }
}

// Read from stdin
let code = '';
process.stdin.setEncoding('utf8');
process.stdin.on('data', chunk => { code += chunk; });
process.stdin.on('end', () => validateTypeScript(code));
```

**Tests**:
- Test valid TypeScript code
- Test invalid TypeScript code (type errors)
- Test syntax errors
- Test ESLint validation
- Test timeout handling
- Test cleanup

---

### **Task 9: Update Tests**
**Priority**: P1
**Estimated Time**: 3-4 days

**Update/Create**:
- `backend/tests/generation/test_generator_service.py` (major rewrite)
- `backend/tests/generation/test_llm_generator.py` (new)
- `backend/tests/generation/test_code_validator.py` (new)
- `backend/tests/generation/test_prompt_builder.py` (new)

**Acceptance Criteria**:

**test_generator_service.py** (rewrite):
- [ ] Test new simplified pipeline:
  ```python
  async def test_generate_success_path():
      """Test successful generation with valid code."""
      result = await generator.generate(request)
      assert result.success
      assert result.component_code
      assert result.stories_code
      assert result.metadata.validation_attempts == 0

  async def test_generate_with_validation_fixes():
      """Test generation that needs validation fixes."""
      # Mock LLM to return invalid code first, then valid
      result = await generator.generate(request)
      assert result.success
      assert result.metadata.validation_attempts == 1

  async def test_generate_validation_failure():
      """Test generation that fails validation after retries."""
      result = await generator.generate(request)
      assert not result.success
      assert result.validation_errors
      assert result.metadata.validation_attempts == 2
  ```
- [ ] Test LLM generation with mocks (use `unittest.mock`)
- [ ] Test validation loop convergence
- [ ] Test fix retries (0, 1, 2 attempts)
- [ ] Test error handling (LLM errors, validation errors)
- [ ] Test performance metrics
- [ ] Test LangSmith tracing (mock tracer)

**test_llm_generator.py** (new):
- [ ] Test prompt building
- [ ] Test structured output parsing
- [ ] Test error handling (API errors, malformed responses)
- [ ] Test token usage tracking
- [ ] Test retry logic
- [ ] Mock OpenAI API calls

**test_code_validator.py** (new):
- [ ] Test TypeScript validation:
  - Valid code passes
  - Invalid code detected
  - Syntax errors handled
- [ ] Test ESLint validation:
  - Valid code passes
  - Lint errors detected
- [ ] Test fix loop logic:
  - Converges after 1 fix
  - Converges after 2 fixes
  - Fails after max retries
- [ ] Test quality scoring
- [ ] Test parallel validation
- [ ] Mock Node.js subprocess calls

**test_prompt_builder.py** (new):
- [ ] Test prompt construction:
  - Pattern formatting
  - Token formatting
  - Requirements formatting
- [ ] Test exemplar selection:
  - Correct type selection
  - Fallback logic
  - Token budget
- [ ] Test token counting
- [ ] Test prompt versioning

**Integration Tests**:
- [ ] End-to-end generation with real LLM (marked as `@pytest.mark.integration`)
- [ ] Validation success cases
- [ ] Validation failure + fix cases
- [ ] Performance benchmarks
- [ ] Quality metrics

**Test Coverage**:
- [ ] Maintain overall coverage >90%
- [ ] New modules: >95% coverage
- [ ] Integration tests for critical paths

**Estimated Time**: 3-4 days

---

### **Task 10: Update API Endpoints**
**Priority**: P1
**Estimated Time**: 1 day

**Update**: `backend/src/api/v1/routes/generation.py`

**Acceptance Criteria**:
- [ ] Update response schema to include:
  ```python
  class GenerationResponse(BaseModel):
      success: bool
      component_code: Optional[str]
      stories_code: Optional[str]
      files: Dict[str, str]
      metadata: GenerationMetadata
      validation_results: Optional[ValidationResults]  # New
      quality_scores: Optional[QualityScores]  # New
      error: Optional[str]

  class ValidationResults(BaseModel):
      attempts: int
      final_status: str  # "passed", "failed", "skipped"
      typescript_errors: List[ValidationError]
      eslint_errors: List[ValidationError]

  class QualityScores(BaseModel):
      compilation: bool
      linting: int  # 0-100
      type_safety: int  # 0-100
      overall: int  # 0-100
  ```
- [ ] Add streaming support (optional):
  - Stream generation progress (`GENERATING`, `VALIDATING`, `POST_PROCESSING`)
  - Stream validation status
  - Use Server-Sent Events (SSE)
- [ ] Error handling improvements:
  - LLM API failures (clear error messages)
  - Validation failures (show errors)
  - Timeout errors (graceful degradation)
  - Rate limiting (429 errors)
- [ ] Update OpenAPI docs:
  - New response schema
  - Validation results
  - Quality scores
  - Error codes
- [ ] Add feature flag support:
  - `ENABLE_LLM_GENERATION` env var
  - Fall back to old pipeline if disabled
  - A/B testing support

**Tests**:
- Test successful generation response
- Test validation failure response
- Test error handling
- Test streaming (if implemented)
- Test OpenAPI schema validation

---

### **Task 11: Frontend Updates**
**Priority**: P2 (Can wait until backend complete)
**Estimated Time**: 2 days

**Update**:
- `app/src/app/preview/page.tsx`
- `app/src/components/composite/GenerationProgress.tsx`
- `app/src/types/generation.types.ts`

**Acceptance Criteria**:
- [ ] Update progress stages (3 stages now):
  - ‚úÖ Generating (LLM generation)
  - ‚úÖ Validating (TypeScript + ESLint)
  - ‚úÖ Post-Processing (Formatting)
- [ ] Show validation results in UI:
  - ‚úÖ TypeScript compilation status
  - ‚úÖ ESLint validation status
  - ‚ö†Ô∏è  Warnings count
  - üîß Fix attempts (if any)
- [ ] Show quality scores:
  - Overall quality score: 8.5/10
  - Type safety: 95/100
  - Linting: 90/100
  - Accessibility: 100/100
- [ ] Show fix attempts indicator:
  - "‚úì Generated perfectly on first try"
  - "üîß Fixed 1 issue automatically"
  - "üîß Fixed 2 issues after validation"
- [ ] Error messages more detailed:
  - Show validation errors with line numbers
  - Suggest fixes or regeneration
- [ ] Quality indicators on Quality tab:
  ```tsx
  <div className="space-y-4">
    <QualityMetric
      label="TypeScript Compilation"
      status={metadata.validation.typescript_passed ? "pass" : "fail"}
      details={metadata.validation.typescript_errors}
    />
    <QualityMetric
      label="ESLint Validation"
      status={metadata.validation.eslint_passed ? "pass" : "fail"}
      details={`${metadata.validation.eslint_warnings} warnings`}
    />
    <QualityScore
      label="Overall Quality"
      score={metadata.quality_scores.overall}
      breakdown={{
        "Type Safety": metadata.quality_scores.type_safety,
        "Code Quality": metadata.quality_scores.linting,
        "Accessibility": metadata.quality_scores.accessibility
      }}
    />
  </div>
  ```
- [ ] Update types to match new API response

**Tests**:
- Test UI rendering with validation results
- Test quality score display
- Test error message display
- Test fix attempts indicator

---

### **Task 12: Cleanup - Delete Old Modules** üóëÔ∏è
**Priority**: P2 (After Tasks 1-7 complete)
**Estimated Time**: 1 day

**Delete Files** (12 files total):

**Backend Modules** (6 files):
```bash
‚ùå backend/src/generation/token_injector.py
‚ùå backend/src/generation/tailwind_generator.py
‚ùå backend/src/generation/requirement_implementer.py
‚ùå backend/src/generation/a11y_enhancer.py
‚ùå backend/src/generation/type_generator.py
‚ùå backend/src/generation/storybook_generator.py
```

**Test Files** (6 files):
```bash
‚ùå backend/tests/generation/test_token_injector.py
‚ùå backend/tests/generation/test_tailwind_generator.py
‚ùå backend/tests/generation/test_requirement_implementer.py
‚ùå backend/tests/generation/test_a11y_enhancer.py
‚ùå backend/tests/generation/test_type_generator.py
‚ùå backend/tests/generation/test_storybook_generator.py
```

**Keep Files** (4 files + 2 tests):
```bash
‚úÖ backend/src/generation/provenance.py          # Traceability
‚úÖ backend/src/generation/import_resolver.py     # Import cleanup
‚úÖ backend/src/generation/types.py               # Type definitions (update)
‚úÖ backend/src/generation/README.md              # Documentation (update)
‚úÖ backend/tests/generation/test_provenance.py
‚úÖ backend/tests/generation/test_import_resolver.py
```

**Acceptance Criteria**:
- [ ] Delete all 12 files listed above
- [ ] Remove imports from `generator_service.py`
- [ ] Update `types.py` with new schemas
- [ ] Update README.md with new architecture
- [ ] Verify all tests still pass
- [ ] Check for any remaining references (use `grep`)
- [ ] Update `.gitignore` if needed

**Commands**:
```bash
# Delete old modules
rm backend/src/generation/{token_injector,tailwind_generator,requirement_implementer,a11y_enhancer,type_generator,storybook_generator}.py

# Delete old tests
rm backend/tests/generation/test_{token_injector,tailwind_generator,requirement_implementer,a11y_enhancer,type_generator,storybook_generator}.py

# Check for references
grep -r "token_injector\|tailwind_generator\|requirement_implementer" backend/

# Run tests
pytest backend/tests/generation/ -v
```

---

### **Task 13: Update Documentation**
**Priority**: P2
**Estimated Time**: 1-2 days

**Update Files**:
- `backend/src/generation/README.md`
- `.claude/epics/04-code-generation.md`
- `CLAUDE.md` (if needed)

**Create Files**:
- `backend/src/generation/PROMPTING_GUIDE.md` (new)
- `backend/src/generation/TROUBLESHOOTING.md` (new)

**Acceptance Criteria**:

**README.md**:
- [ ] Update architecture diagram (8 stages ‚Üí 3 stages)
- [ ] Document LLM-first approach
- [ ] Update module list (remove old, add new)
- [ ] Update performance targets
- [ ] Update usage examples
- [ ] Add troubleshooting section

**04-code-generation.md**:
- [ ] Mark Epic 4 as complete
- [ ] Reference Epic 4.5 for improvements
- [ ] Document migration path
- [ ] Update success metrics

**PROMPTING_GUIDE.md** (new):
- [ ] System prompt template
- [ ] User prompt structure
- [ ] Exemplar format
- [ ] Few-shot learning strategy
- [ ] Token optimization tips
- [ ] Prompt versioning guide
- [ ] Testing prompts
- [ ] A/B testing strategy

**TROUBLESHOOTING.md** (new):
- [ ] Common issues and solutions:
  - LLM returns invalid code
  - Validation fails repeatedly
  - Slow generation time
  - High LLM costs
- [ ] Debugging with LangSmith
- [ ] Log analysis
- [ ] Performance profiling
- [ ] Quality debugging

---

### **Task 14: Performance Optimization**
**Priority**: P2 (After core functionality works)
**Estimated Time**: 2-3 days

**Acceptance Criteria**:
- [ ] **Parallel validation** (TypeScript + ESLint):
  - Run both validations simultaneously
  - Aggregate results
  - Target: 3-5s validation time
- [ ] **Prompt caching**:
  - Cache system prompt (static)
  - Cache exemplars (rarely change)
  - OpenAI prompt caching API
- [ ] **Lazy load exemplars**:
  - Load on first use
  - Keep in memory after loading
  - Invalidate cache on updates
- [ ] **Optimize LLM parameters**:
  - Temperature: 0.3-0.5 (balanced)
  - Max tokens: Optimized per call
  - Model: gpt-4o (fast) vs gpt-4 (quality)
  - Test different models for cost/quality tradeoff
- [ ] **Request caching**:
  - Cache identical requests (same pattern + tokens + requirements)
  - TTL: 1 hour
  - Redis or in-memory cache
- [ ] **Target latency**:
  - p50: <30s (from ~60s) ‚úÖ
  - p95: <60s (from ~90s) ‚úÖ
  - p99: <90s ‚úÖ
- [ ] **Token usage optimization**:
  - Compress requirements representation
  - Truncate long patterns if needed
  - Remove redundant information
- [ ] **Monitoring**:
  - Track latency percentiles
  - Track LLM costs per request
  - Track cache hit rates
  - Alert on regressions

**Tests**:
- Performance benchmarks
- Load testing (concurrent requests)
- Cache hit rate testing

---

### **Task 15: Quality Monitoring Dashboard**
**Priority**: P3 (Nice to have)
**Estimated Time**: 2 days

**Create**: `backend/src/monitoring/generation_metrics.py`

**Acceptance Criteria**:
- [ ] Track quality metrics:
  - Generation success rate (%)
  - Validation pass rate (%)
  - Fix success rate by attempt (1st, 2nd)
  - Average quality scores (compilation, linting, type safety)
  - Latency percentiles (p50, p95, p99)
  - LLM token usage and costs
- [ ] LangSmith integration:
  - Custom metadata in traces
  - Quality scores in trace metadata
  - Search/filter by quality
- [ ] Prometheus metrics export:
  ```python
  generation_success_rate = Gauge(
      'generation_success_rate',
      'Percentage of successful generations'
  )

  validation_pass_rate = Gauge(
      'validation_pass_rate',
      'Percentage of validations that pass'
  )

  generation_latency = Histogram(
      'generation_latency_seconds',
      'Generation latency distribution'
  )
  ```
- [ ] Alert thresholds:
  - Success rate <90% ‚Üí Warning
  - Success rate <80% ‚Üí Critical
  - p95 latency >60s ‚Üí Warning
  - Fix rate >30% ‚Üí Warning (too many fixes needed)
- [ ] Weekly quality reports:
  - Email summary
  - Quality trends
  - Top errors/issues
  - Cost analysis
- [ ] Dashboard (optional):
  - Grafana dashboard JSON
  - Real-time metrics
  - Quality trends over time

---

## File Changes Summary

### üìä Statistics
- **New Files**: 7
- **Updated Files**: 6
- **Deleted Files**: 12
- **Net Change**: -5 files (19 ‚Üí 14)
- **LOC Reduction**: ~50% (~2800 ‚Üí ~1500)

### ‚ú® New Files (7)
```
backend/src/generation/llm_generator.py           (~300 LOC)
backend/src/generation/code_validator.py          (~400 LOC)
backend/src/generation/prompt_builder.py          (~250 LOC)
backend/src/generation/exemplar_loader.py         (~150 LOC)
backend/scripts/validate_typescript.js            (~150 LOC)
backend/scripts/validate_eslint.js                (~100 LOC)
backend/tests/generation/test_llm_generator.py    (~200 LOC)
backend/tests/generation/test_code_validator.py   (~200 LOC)
backend/tests/generation/test_prompt_builder.py   (~150 LOC)
```

### üìù Updated Files (6)
```
backend/src/generation/generator_service.py       (major refactor)
backend/src/generation/pattern_parser.py          (simplify)
backend/src/generation/code_assembler.py          (simplify)
backend/src/generation/types.py                   (new schemas)
backend/src/api/v1/routes/generation.py           (update response)
backend/tests/generation/test_generator_service.py (rewrite)
```

### ‚ùå Deleted Files (12)
```
backend/src/generation/token_injector.py          (~250 LOC)
backend/src/generation/tailwind_generator.py      (~200 LOC)
backend/src/generation/requirement_implementer.py (~310 LOC)
backend/src/generation/a11y_enhancer.py           (~348 LOC)
backend/src/generation/type_generator.py          (~315 LOC)
backend/src/generation/storybook_generator.py     (~250 LOC)
backend/tests/generation/test_token_injector.py
backend/tests/generation/test_tailwind_generator.py
backend/tests/generation/test_requirement_implementer.py
backend/tests/generation/test_a11y_enhancer.py
backend/tests/generation/test_type_generator.py
backend/tests/generation/test_storybook_generator.py
```

### ‚úÖ Kept Files (6)
```
backend/src/generation/provenance.py              (no changes)
backend/src/generation/import_resolver.py         (no changes)
backend/src/generation/types.py                   (update only)
backend/src/generation/README.md                  (update docs)
backend/tests/generation/test_provenance.py       (no changes)
backend/tests/generation/test_import_resolver.py  (no changes)
```

---

## Dependencies

### Existing Dependencies (Already have)
```python
# requirements.txt
openai>=1.0.0          # Already present
langchain>=0.1.0       # Already present
langsmith>=0.1.0       # Already present
fastapi>=0.100.0       # Already present
pydantic>=2.0.0        # Already present
```

### New Dependencies (Add to requirements.txt)
```python
tiktoken>=0.5.0        # Token counting for prompt optimization
```

### Node.js Dependencies (Already in app/package.json)
```json
{
  "typescript": "5.9.3",
  "eslint": "^9",
  "@typescript-eslint/parser": "^8"
}
```

---

## Success Metrics

### Quality Metrics
| Metric | Before | Target | Measurement |
|--------|--------|--------|-------------|
| **TypeScript Compilation Success** | ~60% | 95%+ | Validation pass rate |
| **Code Quality Score** | 5/10 | 8-9/10 | Composite quality metric |
| **User Regeneration Rate** | High | <10% | Usage analytics |
| **Zero Type Errors** | 60% | 95%+ | Validation results |
| **Zero Lint Errors** | 70% | 95%+ | Validation results |

### Performance Metrics
| Metric | Before | Target | Measurement |
|--------|--------|--------|-------------|
| **Generation Latency (p50)** | ~60s | <30s | LangSmith traces |
| **Generation Latency (p95)** | ~90s | <60s | LangSmith traces |
| **Generation Latency (p99)** | ~120s | <90s | LangSmith traces |
| **Validation Latency** | N/A | <5s | Validation timing |
| **First-Try Success Rate** | ~60% | 80%+ | Validation attempts |

### Codebase Metrics
| Metric | Before | Target | Measurement |
|--------|--------|--------|-------------|
| **Lines of Code (generation module)** | ~2800 | ~1500 | Code analysis |
| **Number of Modules** | 11 | 7 | File count |
| **Test Coverage** | ~85% | >90% | pytest-cov |
| **Cyclomatic Complexity** | High | Medium | Code analysis |

---

## Rollout Strategy

### Phase 1: Core Implementation (Week 1-2)
**Tasks**: 1, 2, 3, 5, 8

**Goals**:
- [ ] LLM generator working
- [ ] Validation loop working
- [ ] Simplified generator service
- [ ] Feature flag: `ENABLE_LLM_GENERATION=false` (testing only)

**Testing**:
- Unit tests for new modules
- Integration test with Button pattern
- Compare quality vs old pipeline

**Success Criteria**:
- Button component generates successfully
- Validation catches errors and fixes them
- Quality score >7/10

---

### Phase 2: Quality & Testing (Week 2-3)
**Tasks**: 4, 6, 7, 9

**Goals**:
- [ ] Exemplar library complete (5 exemplars)
- [ ] Pattern parser simplified
- [ ] Code assembler simplified
- [ ] All tests passing

**Testing**:
- Test with all pattern types (Button, Card, Input, etc.)
- A/B testing: old vs new pipeline
- Performance benchmarking

**Success Criteria**:
- All patterns generate successfully
- Quality score >8/10 on average
- Test coverage >90%

---

### Phase 3: Cleanup & Production (Week 3-4)
**Tasks**: 10, 11, 12, 13

**Goals**:
- [ ] API updated with new response schema
- [ ] Frontend displays validation results
- [ ] Old modules deleted
- [ ] Documentation updated

**Testing**:
- End-to-end testing
- User acceptance testing
- Performance testing under load

**Success Criteria**:
- Production deployment successful
- No regressions
- User feedback positive

---

### Phase 4: Optimization (Week 4+)
**Tasks**: 14, 15

**Goals**:
- [ ] Performance optimized (p50 <30s)
- [ ] Monitoring dashboard live
- [ ] Cost optimization

**Testing**:
- Load testing
- Cost analysis
- Quality trending

**Success Criteria**:
- Latency targets met
- Quality sustained >8/10
- Costs acceptable

---

## Risks & Mitigation

| Risk | Impact | Probability | Mitigation |
|------|--------|-------------|------------|
| **LLM generates invalid code** | High | Medium | Validation loop with max 2 fixes, detailed error reporting |
| **LLM latency too high** | Medium | Low | Optimize prompts, parallel validation, caching |
| **LLM costs too high** | Medium | Medium | Prompt optimization, caching, model selection (gpt-4o) |
| **Quality regression** | High | Low | A/B testing, gradual rollout, feature flag |
| **Breaking existing users** | High | Low | Feature flag, backward compatibility during transition |
| **Validation scripts fail** | High | Low | Graceful degradation, skip validation if unavailable |
| **Fix loop doesn't converge** | Medium | Medium | Max retry limit (2), return detailed errors |
| **Team resistance to LLM approach** | Medium | Low | Show quality improvements, demo working system |

---

## Definition of Done

- [ ] All 15 tasks completed with acceptance criteria met
- [ ] LLM generator creates complete, coherent components
- [ ] Validation loop catches and fixes errors (80%+ success)
- [ ] TypeScript compilation success ‚â•95%
- [ ] Code quality score ‚â•8/10
- [ ] Generation latency: p50 <30s, p95 <60s
- [ ] All 12 old modules deleted
- [ ] Test coverage >90%
- [ ] Documentation updated
- [ ] Production deployment successful
- [ ] No critical bugs
- [ ] User feedback positive
- [ ] Monitoring dashboard showing quality metrics

---

## Dependencies & Blockers

### Depends On
- ‚úÖ Epic 4 (Current Code Generation) - Complete
- ‚úÖ OpenAI API access - Have
- ‚úÖ LangSmith setup - Have
- ‚úÖ Node.js installed - Have

### Blocks
- Epic 5 (Quality Validation) - Can leverage validation loop
- Epic 8 (Regeneration) - Will use same LLM pipeline

---

## Related Epics

- **Epic 4**: Current code generation (completed, being replaced)
- **Epic 5**: Quality validation (can leverage validation loop from Task 2)
- **Epic 8**: Regeneration & versioning (will use same LLM pipeline)

---

## References

- [Epic 4 Specification](./04-code-generation.md)
- [Epic 5 Specification](./05-quality-validation.md)
- [OpenAI Structured Outputs](https://platform.openai.com/docs/guides/structured-outputs)
- [LangSmith Tracing](https://docs.smith.langchain.com/)
- [TypeScript Compiler API](https://github.com/microsoft/TypeScript/wiki/Using-the-Compiler-API)
- [ESLint Node.js API](https://eslint.org/docs/latest/integrate/nodejs-api)

---

## Notes

### Why LLM-First?

**Current approach problems**:
- 8 disconnected stages ‚Üí fragmented output
- Regex manipulation ‚Üí brittle, error-prone
- No validation ‚Üí ships broken code
- Hard to maintain ‚Üí 2800 LOC of complex logic

**LLM-first benefits**:
- Single coherent generation ‚Üí better quality
- Semantic understanding ‚Üí natural integration
- Self-healing via validation ‚Üí guaranteed working code
- Simpler architecture ‚Üí easier to maintain and improve

### Cost Considerations

**Estimated costs per component**:
- Generation: ~8000 tokens input, ~2000 tokens output
- Cost: ~$0.10-0.15 per component (gpt-4o)
- With caching: ~$0.05-0.08 per component
- Fix attempts: +$0.05 per fix

**Mitigation**:
- Use gpt-4o (cheaper, faster than gpt-4)
- Implement prompt caching
- Cache identical requests
- Optimize prompt length

### Future Enhancements

- Fine-tuned model for component generation
- Multi-framework support (Vue, Angular, Svelte)
- Visual preview generation
- Component testing generation
- Performance optimization suggestions
- Automatic documentation generation
