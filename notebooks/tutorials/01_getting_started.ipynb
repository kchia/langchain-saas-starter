{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started with Component Forge\n",
    "\n",
    "Welcome to Component Forge! This notebook will guide you through the basics of working with the AI development environment.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "1. Environment setup and imports\n",
    "2. Database connection and basic queries\n",
    "3. Working with sample data\n",
    "4. Basic AI operations\n",
    "5. Data visualization\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Docker services running (`docker-compose up -d`)\n",
    "- Database migrated (`alembic upgrade head`)\n",
    "- Sample data loaded (`python scripts/seed_all.py`)\n",
    "- Jupyter kernel installed (`./scripts/setup_jupyter.sh`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import sys\n",
    "import os\n",
    "import asyncio\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Add project paths\n",
    "project_root = Path.cwd().parent if Path.cwd().name == 'notebooks' else Path.cwd()\n",
    "backend_src = project_root / 'backend' / 'src'\n",
    "sys.path.insert(0, str(backend_src))\n",
    "\n",
    "# Load environment variables\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(project_root / 'backend' / '.env')\n",
    "\n",
    "print(f\"📁 Project root: {project_root}\")\n",
    "print(f\"🐍 Python path includes: {backend_src}\")\n",
    "print(f\"✅ Environment loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data science imports\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Project imports\n",
    "from core.database import check_database_connection, database_health_check\n",
    "from core.models import User, Document, Conversation, Message\n",
    "from notebooks.utils.database_helpers import (\n",
    "    sync_get_database_stats,\n",
    "    sync_get_users_df,\n",
    "    sync_get_documents_df,\n",
    "    sync_get_conversations_df\n",
    ")\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "\n",
    "print(\"📊 Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Database Connection Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test database connection\n",
    "connection_ok = await check_database_connection()\n",
    "print(f\"🗄️ Database connection: {'✅ OK' if connection_ok else '❌ Failed'}\")\n",
    "\n",
    "if connection_ok:\n",
    "    health = await database_health_check()\n",
    "    print(f\"\\n📊 Database Health:\")\n",
    "    print(f\"  Status: {health['status']}\")\n",
    "    print(f\"  Tables: {health.get('table_count', 'Unknown')}\")\n",
    "    print(f\"  Connection time: {health.get('connection_time_ms', 'Unknown')}ms\")\n",
    "else:\n",
    "    print(\"❌ Please check that Docker services are running and database is migrated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Explore Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get database statistics\n",
    "stats = sync_get_database_stats()\n",
    "\n",
    "print(\"📈 Database Statistics:\")\n",
    "for table, count in stats.items():\n",
    "    print(f\"  {table}: {count} records\")\n",
    "\n",
    "# Visualize data distribution\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "tables = list(stats.keys())\n",
    "counts = list(stats.values())\n",
    "\n",
    "bars = ax.bar(tables, counts, color=sns.color_palette(\"husl\", len(tables)))\n",
    "ax.set_title('Database Record Counts')\n",
    "ax.set_ylabel('Number of Records')\n",
    "ax.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, count in zip(bars, counts):\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height + 0.1,\n",
    "            f'{count}', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Analyze Users and Activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load users data\n",
    "users_df = sync_get_users_df()\n",
    "print(f\"👥 Found {len(users_df)} users\")\n",
    "\n",
    "# Display users table\n",
    "display(users_df[['username', 'email', 'is_active', 'is_admin', 'login_count', 'created_at']])\n",
    "\n",
    "# Visualize user activity\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Login counts\n",
    "ax1.bar(users_df['username'], users_df['login_count'], \n",
    "        color=sns.color_palette(\"viridis\", len(users_df)))\n",
    "ax1.set_title('User Login Counts')\n",
    "ax1.set_ylabel('Login Count')\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# User types\n",
    "user_types = users_df.groupby(['is_admin', 'is_active']).size().reset_index(name='count')\n",
    "user_types['type'] = user_types.apply(\n",
    "    lambda x: f\"{'Admin' if x['is_admin'] else 'User'} ({'Active' if x['is_active'] else 'Inactive'})\", \n",
    "    axis=1\n",
    ")\n",
    "ax2.pie(user_types['count'], labels=user_types['type'], autopct='%1.0f%%')\n",
    "ax2.set_title('User Types Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Explore Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load documents data\n",
    "docs_df = sync_get_documents_df()\n",
    "print(f\"📄 Found {len(docs_df)} documents\")\n",
    "\n",
    "# Display documents overview\n",
    "display(docs_df[['title', 'content_type', 'category', 'word_count', 'processing_status']])\n",
    "\n",
    "# Document analysis\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Content types\n",
    "content_counts = docs_df['content_type'].value_counts()\n",
    "ax1.pie(content_counts.values, labels=content_counts.index, autopct='%1.0f%%')\n",
    "ax1.set_title('Document Content Types')\n",
    "\n",
    "# Categories\n",
    "category_counts = docs_df['category'].value_counts()\n",
    "ax2.bar(category_counts.index, category_counts.values)\n",
    "ax2.set_title('Document Categories')\n",
    "ax2.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Word count distribution\n",
    "ax3.hist(docs_df['word_count'], bins=10, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "ax3.set_title('Word Count Distribution')\n",
    "ax3.set_xlabel('Word Count')\n",
    "ax3.set_ylabel('Number of Documents')\n",
    "\n",
    "# File sizes\n",
    "docs_df['file_size_kb'] = docs_df['file_size'] / 1024\n",
    "ax4.scatter(docs_df['word_count'], docs_df['file_size_kb'], alpha=0.7)\n",
    "ax4.set_title('Word Count vs File Size')\n",
    "ax4.set_xlabel('Word Count')\n",
    "ax4.set_ylabel('File Size (KB)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Document statistics\n",
    "print(\"\\n📊 Document Statistics:\")\n",
    "print(f\"  Total words: {docs_df['word_count'].sum():,}\")\n",
    "print(f\"  Average words per document: {docs_df['word_count'].mean():.0f}\")\n",
    "print(f\"  Total file size: {docs_df['file_size'].sum() / 1024:.1f} KB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Analyze Conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load conversations data\n",
    "conversations_df = sync_get_conversations_df()\n",
    "print(f\"💬 Found {len(conversations_df)} conversations\")\n",
    "\n",
    "if len(conversations_df) > 0:\n",
    "    # Display conversations overview\n",
    "    display(conversations_df[['username', 'title', 'model_name', 'message_count', 'total_tokens_used']])\n",
    "\n",
    "    # Conversation analysis\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "    # Conversations by user\n",
    "    user_conv_counts = conversations_df['username'].value_counts()\n",
    "    ax1.bar(user_conv_counts.index, user_conv_counts.values)\n",
    "    ax1.set_title('Conversations by User')\n",
    "    ax1.tick_params(axis='x', rotation=45)\n",
    "\n",
    "    # Models used\n",
    "    model_counts = conversations_df['model_name'].value_counts()\n",
    "    ax2.pie(model_counts.values, labels=model_counts.index, autopct='%1.0f%%')\n",
    "    ax2.set_title('Models Used')\n",
    "\n",
    "    # Message count distribution\n",
    "    ax3.hist(conversations_df['message_count'], bins=10, alpha=0.7, color='lightgreen', edgecolor='black')\n",
    "    ax3.set_title('Message Count Distribution')\n",
    "    ax3.set_xlabel('Messages per Conversation')\n",
    "    ax3.set_ylabel('Number of Conversations')\n",
    "\n",
    "    # Token usage\n",
    "    ax4.scatter(conversations_df['message_count'], conversations_df['total_tokens_used'], alpha=0.7)\n",
    "    ax4.set_title('Messages vs Token Usage')\n",
    "    ax4.set_xlabel('Message Count')\n",
    "    ax4.set_ylabel('Total Tokens Used')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Token statistics\n",
    "    print(\"\\n🎯 Token Usage Statistics:\")\n",
    "    print(f\"  Total tokens used: {conversations_df['total_tokens_used'].sum():,}\")\n",
    "    print(f\"  Average tokens per conversation: {conversations_df['total_tokens_used'].mean():.0f}\")\n",
    "    print(f\"  Average messages per conversation: {conversations_df['message_count'].mean():.1f}\")\n",
    "else:\n",
    "    print(\"No conversations found. Run the fixture loader to add sample conversations.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Quick Database Query Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from notebooks.utils.database_helpers import sync_execute_query\n",
    "\n",
    "# Example: Find most active users\n",
    "active_users_query = \"\"\"\n",
    "SELECT \n",
    "    u.username,\n",
    "    COUNT(DISTINCT c.id) as conversation_count,\n",
    "    COUNT(m.id) as total_messages,\n",
    "    SUM(CASE WHEN m.role = 'user' THEN 1 ELSE 0 END) as user_messages,\n",
    "    SUM(m.total_tokens) as total_tokens\n",
    "FROM users u\n",
    "LEFT JOIN conversations c ON u.id = c.user_id\n",
    "LEFT JOIN messages m ON c.id = m.conversation_id\n",
    "GROUP BY u.id, u.username\n",
    "HAVING COUNT(DISTINCT c.id) > 0\n",
    "ORDER BY conversation_count DESC\n",
    "\"\"\"\n",
    "\n",
    "active_users = sync_execute_query(active_users_query)\n",
    "print(\"👑 Most Active Users:\")\n",
    "display(active_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Document categories and their average word counts\n",
    "doc_categories_query = \"\"\"\n",
    "SELECT \n",
    "    category,\n",
    "    COUNT(*) as document_count,\n",
    "    AVG(word_count) as avg_word_count,\n",
    "    SUM(word_count) as total_words\n",
    "FROM documents\n",
    "WHERE processing_status = 'completed'\n",
    "GROUP BY category\n",
    "ORDER BY document_count DESC\n",
    "\"\"\"\n",
    "\n",
    "doc_categories = sync_execute_query(doc_categories_query)\n",
    "print(\"📚 Document Categories Analysis:\")\n",
    "display(doc_categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Next Steps\n",
    "\n",
    "Congratulations! You've successfully explored the Component Forge environment. Here's what you can do next:\n",
    "\n",
    "### 🧪 Experiments\n",
    "- **RAG System Testing**: `notebooks/experiments/02_rag_testing.ipynb`\n",
    "- **Embedding Models**: `notebooks/experiments/03_embedding_comparison.ipynb`\n",
    "- **Prompt Engineering**: `notebooks/experiments/04_prompt_engineering.ipynb`\n",
    "\n",
    "### 📊 Analysis\n",
    "- **Performance Analysis**: `notebooks/evaluation/01_performance_analysis.ipynb`\n",
    "- **User Behavior**: `notebooks/exploration/02_user_behavior.ipynb`\n",
    "- **Token Usage**: `notebooks/exploration/03_token_analysis.ipynb`\n",
    "\n",
    "### 🛠️ Development\n",
    "- Explore the database models in `backend/src/core/models.py`\n",
    "- Try the database helpers in `notebooks/utils/database_helpers.py`\n",
    "- Set up your own experiments using this notebook as a template\n",
    "\n",
    "### 📚 Resources\n",
    "- Project documentation: `CLAUDE.md`\n",
    "- Database design: `backend/data/fixtures/documents/database_design_principles.md`\n",
    "- AI development guide: `backend/data/fixtures/documents/ai_development_guide.md`\n",
    "\n",
    "Happy experimenting! 🚀"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Component Forge",
   "language": "python",
   "name": "component-forge"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}