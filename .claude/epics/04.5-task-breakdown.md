# Epic 4.5: Task Breakdown by Type

**Epic**: LLM-First Code Generation Refactor
**Status**: Not Started
**Total Tasks**: 15
**Estimated Timeline**: 4-5 weeks

---

## üìä Task Overview

| Category | Tasks | Estimated Time | Priority |
|----------|-------|----------------|----------|
| **Backend Core** | 7 tasks | 14-18 days | P0-P1 |
| **Frontend** | 1 task | 2 days | P2 |
| **Integration & Testing** | 4 tasks | 6-9 days | P1-P2 |
| **Optimization & Monitoring** | 2 tasks | 4-5 days | P2-P3 |
| **Cleanup & Docs** | 1 task | 2-3 days | P2 |

---

## üîß Backend Core Tasks

### Task 1: Build LLM Code Generator (Core) ‚≠ê
**Priority**: P0 (Critical Path)
**Estimated Time**: 3-4 days
**Owner**: Backend/AI Team
**Dependencies**: None

**Create**: `backend/src/generation/llm_generator.py`

**Acceptance Criteria**:
- [ ] `LLMComponentGenerator` class with structured output
- [ ] Comprehensive prompt builder that includes:
  - Pattern code as reference example
  - Design tokens with semantic descriptions
  - All requirements (props, events, states, a11y)
  - Component naming and conventions
  - TypeScript strict mode requirements
  - Accessibility requirements
  - shadcn/ui style guidelines
- [ ] Use OpenAI structured outputs or JSON mode
- [ ] Generate complete component code (not fragments)
- [ ] Generate Storybook stories in same call
- [ ] LangSmith tracing for all LLM calls
- [ ] Error handling with retries (3 attempts)
- [ ] Response validation (check for required fields)
- [ ] Token usage tracking and optimization

**Output Schema**:
```python
@dataclass
class LLMGeneratedCode:
    component_code: str      # Complete .tsx file content
    stories_code: str        # Complete .stories.tsx content
    imports: List[str]       # List of import statements
    exports: List[str]       # List of exported names
    explanation: str         # Why code was generated this way
    token_usage: Dict        # Prompt/completion tokens used
```

**Testing**:
- Test prompt construction
- Test structured output parsing
- Test error handling
- Test token usage tracking
- Test with various component types

---

### Task 2: Build Code Validator with Fix Loop ‚≠ê
**Priority**: P0 (Critical Path)
**Estimated Time**: 3-4 days
**Owner**: Backend/AI Team
**Dependencies**: Task 8 (Validation Scripts)

**Create**: `backend/src/generation/code_validator.py`

**Acceptance Criteria**:
- [ ] TypeScript validation using `ts.createProgram` API (via Node.js subprocess)
- [ ] ESLint validation via Node.js subprocess
- [ ] Validation result parsing (errors, warnings, line numbers)
- [ ] LLM-based fix generator that:
  - Takes original code + validation errors
  - Generates fixed code with explanation
  - Preserves working parts
  - Only fixes specific issues
- [ ] Iterative fix loop (max 2-3 attempts)
- [ ] Quality scoring:
  - Compilation success: boolean
  - Lint error count: int
  - Lint warning count: int
  - Type coverage: percentage
- [ ] Performance tracking (<5s validation time)
- [ ] Graceful degradation if validation unavailable
- [ ] Parallel validation (TypeScript + ESLint simultaneously)

**Testing**:
- Test TypeScript validation (valid/invalid code)
- Test ESLint validation
- Test fix generation
- Test fix loop convergence
- Test quality scoring
- Test parallel validation performance

---

### Task 3: Build Prompt Builder ‚≠ê
**Priority**: P0 (Critical Path)
**Estimated Time**: 2-3 days
**Owner**: Backend/AI Team
**Dependencies**: None

**Create**: `backend/src/generation/prompt_builder.py`

**Acceptance Criteria**:
- [ ] `PromptBuilder` class with template management
- [ ] System prompt for component generation
- [ ] Dynamic prompt construction from:
  - Pattern reference (format as example)
  - Design tokens (explain semantic meaning)
  - Requirements (structured by category)
  - Component name and type
  - Exemplars (few-shot learning)
- [ ] Few-shot examples library:
  - Load 2-3 relevant exemplars based on component type
  - Include input (requirements) and output (code)
- [ ] Anti-pattern examples (what NOT to do):
  - Using `any` types
  - Missing ARIA attributes
  - Hardcoded colors instead of tokens
- [ ] Token usage optimization:
  - Truncate long patterns if needed
  - Compress requirements representation
  - Target: <8000 tokens per prompt
- [ ] Prompt versioning for A/B testing
- [ ] Validation constraints in prompt:
  - Zero `any` types
  - All props must have types
  - ARIA attributes required
  - Proper import structure
  - Component displayName required

**Testing**:
- Test prompt construction
- Test token counting
- Test exemplar selection
- Test prompt versioning
- Test with various component types

---

### Task 4: Create Exemplar Library
**Priority**: P1 (Required for quality)
**Estimated Time**: 2-3 days
**Owner**: Backend/AI Team
**Dependencies**: None

**Create**:
- `backend/data/exemplars/` directory
- `backend/src/generation/exemplar_loader.py`

**Acceptance Criteria**:
- [ ] 5 hand-crafted exemplar components:
  - **Button**: variants, sizes, accessibility, icons
  - **Card**: composition, semantic HTML, proper structure
  - **Input**: validation, error states, labels, hints
  - **Checkbox**: controlled, accessibility, labels
  - **Alert**: ARIA live regions, icons, variants
- [ ] Each exemplar includes:
  - `input.json`: Requirements and tokens
  - `output.tsx`: Perfect generated component
  - `output.stories.tsx`: Storybook stories
  - `metadata.json`: Why this is a good example
  - `comments.md`: Explain key decisions
- [ ] JSON metadata for each exemplar
- [ ] `ExemplarLoader` class:
  - Load exemplars from disk
  - Select relevant exemplars by component type
  - Format for prompt inclusion
  - Cache loaded exemplars
- [ ] Selection logic:
  - Prefer same component type (Button ‚Üí Button)
  - Fall back to similar types (Checkbox ‚Üí Switch)
  - Limit to 2-3 examples per prompt (token budget)

**Exemplar Structure**:
```
backend/data/exemplars/
‚îú‚îÄ‚îÄ button/
‚îÇ   ‚îú‚îÄ‚îÄ input.json
‚îÇ   ‚îú‚îÄ‚îÄ output.tsx
‚îÇ   ‚îú‚îÄ‚îÄ output.stories.tsx
‚îÇ   ‚îú‚îÄ‚îÄ metadata.json
‚îÇ   ‚îî‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ card/
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îî‚îÄ‚îÄ input/
    ‚îî‚îÄ‚îÄ ...
```

**Testing**:
- Test exemplar loading
- Test selection logic
- Test formatting for prompts
- Test caching

---

### Task 5: Refactor Generator Service ‚≠ê
**Priority**: P0 (Critical Path)
**Estimated Time**: 2-3 days
**Owner**: Backend/AI Team
**Dependencies**: Tasks 1, 2, 3

**Update**: `backend/src/generation/generator_service.py`

**Acceptance Criteria**:
- [ ] Remove all old stage calls:
  - ‚ùå `_inject_tokens()`
  - ‚ùå `_generate_tailwind()`
  - ‚ùå `_implement_requirements()`
  - ‚ùå `_enhance_accessibility()`
  - ‚ùå `_generate_types()`
  - ‚ùå `_generate_storybook_stories()`
- [ ] New simplified pipeline:
  1. **Load pattern** (as reference only)
  2. **Build generation prompt** (with exemplars)
  3. **Call LLM generator** (single pass)
  4. **Validate generated code** (TypeScript + ESLint)
  5. **If invalid**: LLM fix loop (max 2 retries)
  6. **Post-process**: imports, provenance, formatting
  7. **Return result** with metadata
- [ ] Stage tracking updated to 3 stages:
  - `GENERATING`: LLM generation (~15-20s)
  - `VALIDATING`: TypeScript/ESLint validation (~3-5s)
  - `POST_PROCESSING`: Final assembly (~2-3s)
- [ ] Preserve LangSmith tracing
- [ ] Detailed error reporting
- [ ] Performance metrics
- [ ] Quality metrics in metadata

**Testing**:
- Test successful generation path
- Test validation failure path
- Test fix loop convergence
- Test error handling
- Test performance metrics
- Test LangSmith tracing

---

### Task 6: Update Pattern Parser (Simplify)
**Priority**: P1
**Estimated Time**: 1 day
**Owner**: Backend Team
**Dependencies**: None

**Update**: `backend/src/generation/pattern_parser.py`

**Acceptance Criteria**:
- [ ] Remove modification point detection (no longer needed):
  - ‚ùå `_find_modification_points()`
  - ‚ùå className location tracking
  - ‚ùå variant definition tracking
- [ ] Remove regex-based code analysis:
  - ‚ùå `_extract_props_interface()`
  - ‚ùå `_extract_imports()` (handled by ImportResolver)
- [ ] Keep only basic parsing:
  - ‚úÖ `load_pattern(pattern_id)` - Load JSON
  - ‚úÖ Extract metadata (name, type, variants)
  - ‚úÖ Return pattern code as string (reference only)
- [ ] Add pattern metadata extraction:
  - Component type (button, card, input, etc.)
  - Variants list (for prompt context)
  - Dependencies list
  - A11y features
- [ ] Return simplified `PatternStructure`
- [ ] Performance: <50ms per pattern
- [ ] Error handling for missing patterns

**Testing**:
- Test pattern loading
- Test metadata extraction
- Test error handling
- Test performance

---

### Task 7: Simplify Code Assembler
**Priority**: P1
**Estimated Time**: 1 day
**Owner**: Backend Team
**Dependencies**: None

**Update**: `backend/src/generation/code_assembler.py`

**Acceptance Criteria**:
- [ ] Remove multi-section assembly logic (no longer needed):
  - ‚ùå `_build_code_parts()` from multiple stages
  - ‚ùå Type definitions section
  - ‚ùå CSS variables section (now in code)
  - ‚ùå Component code section
- [ ] Keep only:
  - ‚úÖ **Provenance header injection** (delegate to ProvenanceGenerator)
  - ‚úÖ **Import resolution** (delegate to ImportResolver)
  - ‚úÖ **Prettier formatting** (via Node.js script)
- [ ] Simplified interface
- [ ] Input: complete component code string (not fragments)
- [ ] Output: formatted code with header and organized imports
- [ ] Performance: <2s for formatting

**Testing**:
- Test header injection
- Test import resolution
- Test formatting
- Test error handling

---

### Task 8: Add Validation Scripts (Node.js) ‚≠ê
**Priority**: P0 (Required for Task 2)
**Estimated Time**: 1-2 days
**Owner**: Backend Team
**Dependencies**: None

**Create**:
- `backend/scripts/validate_typescript.js`
- `backend/scripts/validate_eslint.js`

**Acceptance Criteria**:

**TypeScript Validation Script**:
- [ ] Accepts code via stdin
- [ ] Creates temp file in `.tmp/` directory
- [ ] Runs `ts.createProgram` with strict mode config
- [ ] Returns JSON with errors/warnings
- [ ] Cleanup temp files
- [ ] Exit codes: 0 (valid), 1 (errors), 2 (fatal)
- [ ] Performance: <2s for typical component

**ESLint Validation Script**:
- [ ] Accepts code via stdin
- [ ] Runs ESLint programmatically (no temp file needed)
- [ ] Use existing `app/eslint.config.mjs` configuration
- [ ] Returns JSON with errors/warnings
- [ ] Exit codes: 0 (valid), 1 (errors), 2 (fatal)
- [ ] Performance: <2s for typical component

**Both scripts handle edge cases**:
- [ ] Malformed code (syntax errors)
- [ ] Missing dependencies (graceful degradation)
- [ ] Timeout after 10s
- [ ] Proper error messages

**Testing**:
- Test valid TypeScript code
- Test invalid TypeScript code (type errors)
- Test syntax errors
- Test ESLint validation
- Test timeout handling
- Test cleanup

---

### Task 10: Update API Endpoints
**Priority**: P1
**Estimated Time**: 1 day
**Owner**: Backend Team
**Dependencies**: Task 5

**Update**: `backend/src/api/v1/routes/generation.py`

**Acceptance Criteria**:
- [ ] Update response schema to include:
  - `validation_results` (attempts, status, errors)
  - `quality_scores` (compilation, linting, type safety, overall)
- [ ] Add streaming support (optional):
  - Stream generation progress
  - Stream validation status
  - Use Server-Sent Events (SSE)
- [ ] Error handling improvements:
  - LLM API failures (clear error messages)
  - Validation failures (show errors)
  - Timeout errors (graceful degradation)
  - Rate limiting (429 errors)
- [ ] Update OpenAPI docs:
  - New response schema
  - Validation results
  - Quality scores
  - Error codes
- [ ] Add feature flag support:
  - `ENABLE_LLM_GENERATION` env var
  - Fall back to old pipeline if disabled
  - A/B testing support

**Testing**:
- Test successful generation response
- Test validation failure response
- Test error handling
- Test streaming (if implemented)
- Test OpenAPI schema validation

---

## üé® Frontend Tasks

### Task 11: Frontend Updates
**Priority**: P2 (Can wait until backend complete)
**Estimated Time**: 2 days
**Owner**: Frontend Team
**Dependencies**: Task 10

**Update**:
- `app/src/app/preview/page.tsx`
- `app/src/components/composite/GenerationProgress.tsx`
- `app/src/types/generation.types.ts`

**Acceptance Criteria**:
- [ ] Update progress stages (3 stages now):
  - ‚úÖ Generating (LLM generation)
  - ‚úÖ Validating (TypeScript + ESLint)
  - ‚úÖ Post-Processing (Formatting)
- [ ] Show validation results in UI:
  - ‚úÖ TypeScript compilation status
  - ‚úÖ ESLint validation status
  - ‚ö†Ô∏è  Warnings count
  - üîß Fix attempts (if any)
- [ ] Show quality scores:
  - Overall quality score: 8.5/10
  - Type safety: 95/100
  - Linting: 90/100
  - Accessibility: 100/100
- [ ] Show fix attempts indicator:
  - "‚úì Generated perfectly on first try"
  - "üîß Fixed 1 issue automatically"
  - "üîß Fixed 2 issues after validation"
- [ ] Error messages more detailed:
  - Show validation errors with line numbers
  - Suggest fixes or regeneration
- [ ] Quality indicators on Quality tab
- [ ] Update types to match new API response

**Testing**:
- Test UI rendering with validation results
- Test quality score display
- Test error message display
- Test fix attempts indicator

---

## üß™ Integration & Testing Tasks

### Task 9: Update Tests
**Priority**: P1
**Estimated Time**: 3-4 days
**Owner**: Backend Team
**Dependencies**: Tasks 1-8

**Update/Create**:
- `backend/tests/generation/test_generator_service.py` (major rewrite)
- `backend/tests/generation/test_llm_generator.py` (new)
- `backend/tests/generation/test_code_validator.py` (new)
- `backend/tests/generation/test_prompt_builder.py` (new)

**Acceptance Criteria**:

**test_generator_service.py** (rewrite):
- [ ] Test new simplified pipeline
- [ ] Test successful generation with valid code
- [ ] Test generation with validation fixes
- [ ] Test generation validation failure after retries
- [ ] Test LLM generation with mocks
- [ ] Test validation loop convergence
- [ ] Test fix retries (0, 1, 2 attempts)
- [ ] Test error handling (LLM errors, validation errors)
- [ ] Test performance metrics
- [ ] Test LangSmith tracing (mock tracer)

**test_llm_generator.py** (new):
- [ ] Test prompt building
- [ ] Test structured output parsing
- [ ] Test error handling (API errors, malformed responses)
- [ ] Test token usage tracking
- [ ] Test retry logic
- [ ] Mock OpenAI API calls

**test_code_validator.py** (new):
- [ ] Test TypeScript validation (valid/invalid code)
- [ ] Test ESLint validation
- [ ] Test fix loop logic (convergence scenarios)
- [ ] Test quality scoring
- [ ] Test parallel validation
- [ ] Mock Node.js subprocess calls

**test_prompt_builder.py** (new):
- [ ] Test prompt construction (pattern, tokens, requirements)
- [ ] Test exemplar selection
- [ ] Test token counting
- [ ] Test prompt versioning

**Integration Tests**:
- [ ] End-to-end generation with real LLM (marked as `@pytest.mark.integration`)
- [ ] Validation success cases
- [ ] Validation failure + fix cases
- [ ] Performance benchmarks
- [ ] Quality metrics

**Test Coverage**:
- [ ] Maintain overall coverage >90%
- [ ] New modules: >95% coverage
- [ ] Integration tests for critical paths

---

### Task 12: Cleanup - Delete Old Modules üóëÔ∏è
**Priority**: P2 (After Tasks 1-7 complete)
**Estimated Time**: 1 day
**Owner**: Backend Team
**Dependencies**: Tasks 5, 9

**Delete Files** (12 files total):

**Backend Modules** (6 files):
```bash
‚ùå backend/src/generation/token_injector.py
‚ùå backend/src/generation/tailwind_generator.py
‚ùå backend/src/generation/requirement_implementer.py
‚ùå backend/src/generation/a11y_enhancer.py
‚ùå backend/src/generation/type_generator.py
‚ùå backend/src/generation/storybook_generator.py
```

**Test Files** (6 files):
```bash
‚ùå backend/tests/generation/test_token_injector.py
‚ùå backend/tests/generation/test_tailwind_generator.py
‚ùå backend/tests/generation/test_requirement_implementer.py
‚ùå backend/tests/generation/test_a11y_enhancer.py
‚ùå backend/tests/generation/test_type_generator.py
‚ùå backend/tests/generation/test_storybook_generator.py
```

**Keep Files** (4 files + 2 tests):
```bash
‚úÖ backend/src/generation/provenance.py
‚úÖ backend/src/generation/import_resolver.py
‚úÖ backend/src/generation/types.py (update)
‚úÖ backend/src/generation/README.md (update)
‚úÖ backend/tests/generation/test_provenance.py
‚úÖ backend/tests/generation/test_import_resolver.py
```

**Acceptance Criteria**:
- [ ] Delete all 12 files listed above
- [ ] Remove imports from `generator_service.py`
- [ ] Update `types.py` with new schemas
- [ ] Update README.md with new architecture
- [ ] Verify all tests still pass
- [ ] Check for any remaining references (use `grep`)
- [ ] Update `.gitignore` if needed

---

### Task 13: Update Documentation
**Priority**: P2
**Estimated Time**: 1-2 days
**Owner**: Documentation Team
**Dependencies**: Tasks 1-12

**Update Files**:
- `backend/src/generation/README.md`
- `.claude/epics/04-code-generation.md`
- `CLAUDE.md` (if needed)

**Create Files**:
- `backend/src/generation/PROMPTING_GUIDE.md` (new)
- `backend/src/generation/TROUBLESHOOTING.md` (new)

**Acceptance Criteria**:

**README.md**:
- [ ] Update architecture diagram (8 stages ‚Üí 3 stages)
- [ ] Document LLM-first approach
- [ ] Update module list (remove old, add new)
- [ ] Update performance targets
- [ ] Update usage examples
- [ ] Add troubleshooting section

**04-code-generation.md**:
- [ ] Mark Epic 4 as complete
- [ ] Reference Epic 4.5 for improvements
- [ ] Document migration path
- [ ] Update success metrics

**PROMPTING_GUIDE.md** (new):
- [ ] System prompt template
- [ ] User prompt structure
- [ ] Exemplar format
- [ ] Few-shot learning strategy
- [ ] Token optimization tips
- [ ] Prompt versioning guide
- [ ] Testing prompts
- [ ] A/B testing strategy

**TROUBLESHOOTING.md** (new):
- [ ] Common issues and solutions
- [ ] Debugging with LangSmith
- [ ] Log analysis
- [ ] Performance profiling
- [ ] Quality debugging

---

### Task E2E: End-to-End Integration Testing
**Priority**: P1
**Estimated Time**: 1-2 days
**Owner**: QA/Backend Team
**Dependencies**: Tasks 1-11

**Not a separate task in original epic, but critical for success**

**Acceptance Criteria**:
- [ ] Test complete flow: Request ‚Üí Generation ‚Üí Validation ‚Üí Response
- [ ] Test with all pattern types:
  - Button, Card, Input, Checkbox, Alert
- [ ] Test error scenarios:
  - LLM API failures
  - Validation failures
  - Timeout errors
- [ ] Test performance under load:
  - 10 concurrent requests
  - 50 concurrent requests
- [ ] Test A/B comparison (old vs new pipeline):
  - Quality metrics
  - Performance metrics
  - Success rates
- [ ] Test feature flag toggling
- [ ] Test with real OpenAI API (integration environment)
- [ ] Load testing and stress testing

---

## ‚ö° Optimization & Monitoring Tasks

### Task 14: Performance Optimization
**Priority**: P2 (After core functionality works)
**Estimated Time**: 2-3 days
**Owner**: Backend Team
**Dependencies**: Tasks 1-11

**Acceptance Criteria**:
- [ ] **Parallel validation** (TypeScript + ESLint):
  - Run both validations simultaneously
  - Aggregate results
  - Target: 3-5s validation time
- [ ] **Prompt caching**:
  - Cache system prompt (static)
  - Cache exemplars (rarely change)
  - OpenAI prompt caching API
- [ ] **Lazy load exemplars**:
  - Load on first use
  - Keep in memory after loading
  - Invalidate cache on updates
- [ ] **Optimize LLM parameters**:
  - Temperature: 0.3-0.5 (balanced)
  - Max tokens: Optimized per call
  - Model: gpt-4o (fast) vs gpt-4 (quality)
  - Test different models for cost/quality tradeoff
- [ ] **Request caching**:
  - Cache identical requests
  - TTL: 1 hour
  - Redis or in-memory cache
- [ ] **Target latency**:
  - p50: <30s (from ~60s) ‚úÖ
  - p95: <60s (from ~90s) ‚úÖ
  - p99: <90s ‚úÖ
- [ ] **Token usage optimization**:
  - Compress requirements representation
  - Truncate long patterns if needed
  - Remove redundant information
- [ ] **Monitoring**:
  - Track latency percentiles
  - Track LLM costs per request
  - Track cache hit rates
  - Alert on regressions

**Testing**:
- Performance benchmarks
- Load testing (concurrent requests)
- Cache hit rate testing

---

### Task 15: Quality Monitoring Dashboard
**Priority**: P3 (Nice to have)
**Estimated Time**: 2 days
**Owner**: Backend/DevOps Team
**Dependencies**: Task 14

**Create**: `backend/src/monitoring/generation_metrics.py`

**Acceptance Criteria**:
- [ ] Track quality metrics:
  - Generation success rate (%)
  - Validation pass rate (%)
  - Fix success rate by attempt (1st, 2nd)
  - Average quality scores
  - Latency percentiles (p50, p95, p99)
  - LLM token usage and costs
- [ ] LangSmith integration:
  - Custom metadata in traces
  - Quality scores in trace metadata
  - Search/filter by quality
- [ ] Prometheus metrics export:
  - `generation_success_rate`
  - `validation_pass_rate`
  - `generation_latency`
- [ ] Alert thresholds:
  - Success rate <90% ‚Üí Warning
  - Success rate <80% ‚Üí Critical
  - p95 latency >60s ‚Üí Warning
  - Fix rate >30% ‚Üí Warning
- [ ] Weekly quality reports:
  - Email summary
  - Quality trends
  - Top errors/issues
  - Cost analysis
- [ ] Dashboard (optional):
  - Grafana dashboard JSON
  - Real-time metrics
  - Quality trends over time

---

## üó∫Ô∏è Task Dependency Graph

```
Phase 1: Core Implementation (Week 1-2)
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                                         ‚îÇ
‚îÇ  Task 8 (Validation Scripts) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ
‚îÇ         ‚Üì                          ‚Üì    ‚îÇ
‚îÇ  Task 1 (LLM Generator)      Task 2    ‚îÇ
‚îÇ         ‚Üì                    (Validator)‚îÇ
‚îÇ  Task 3 (Prompt Builder)          ‚Üì    ‚îÇ
‚îÇ         ‚Üì                          ‚îÇ    ‚îÇ
‚îÇ  Task 5 (Generator Service) ‚Üê‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ
‚îÇ                                         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Phase 2: Quality & Testing (Week 2-3)
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                                         ‚îÇ
‚îÇ  Task 4 (Exemplars)                    ‚îÇ
‚îÇ  Task 6 (Pattern Parser)               ‚îÇ
‚îÇ  Task 7 (Code Assembler)               ‚îÇ
‚îÇ         ‚Üì                               ‚îÇ
‚îÇ  Task 9 (Tests) ‚Üê‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ                                      ‚îÇ  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ‚îÄ‚îÄ‚îò
                                       ‚îÇ
Phase 3: Cleanup & Production (Week 3-4)‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ‚îÄ‚îÄ‚îê
‚îÇ                                      ‚Üì  ‚îÇ
‚îÇ  Task 10 (API Updates) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí Task 11  ‚îÇ
‚îÇ         ‚Üì                   (Frontend) ‚îÇ
‚îÇ  Task 12 (Cleanup) ‚Üê‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ  Task 13 (Documentation)               ‚îÇ
‚îÇ         ‚Üì                               ‚îÇ
‚îÇ  Task E2E (Integration Testing)        ‚îÇ
‚îÇ                                         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Phase 4: Optimization (Week 4+)
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                                         ‚îÇ
‚îÇ  Task 14 (Performance)                 ‚îÇ
‚îÇ         ‚Üì                               ‚îÇ
‚îÇ  Task 15 (Monitoring)                  ‚îÇ
‚îÇ                                         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üìÖ Recommended Sprint Planning

### Sprint 1 (Week 1): Foundation
**Focus**: Core LLM generation infrastructure

**Backend**:
- Task 8: Validation Scripts (1-2 days) ‚≠ê
- Task 1: LLM Generator (3-4 days) ‚≠ê
- Task 3: Prompt Builder (2-3 days) ‚≠ê

**Deliverables**:
- LLM can generate components
- Basic prompt construction works
- Validation scripts ready

---

### Sprint 2 (Week 2): Integration
**Focus**: Connect all pieces, add validation loop

**Backend**:
- Task 2: Code Validator (3-4 days) ‚≠ê
- Task 5: Generator Service Refactor (2-3 days) ‚≠ê
- Task 4: Exemplar Library (2-3 days)

**Deliverables**:
- End-to-end generation works
- Validation loop fixes errors
- Quality exemplars available

---

### Sprint 3 (Week 3): Simplification & Testing
**Focus**: Clean up old code, comprehensive testing

**Backend**:
- Task 6: Pattern Parser Simplification (1 day)
- Task 7: Code Assembler Simplification (1 day)
- Task 9: Update Tests (3-4 days)
- Task 10: API Updates (1 day)

**Deliverables**:
- Old modules removed
- Test coverage >90%
- API updated with new schema

---

### Sprint 4 (Week 4): Production & Polish
**Focus**: Frontend, docs, deployment

**Frontend**:
- Task 11: Frontend Updates (2 days)

**Integration**:
- Task E2E: End-to-End Testing (1-2 days)
- Task 12: Cleanup (1 day)
- Task 13: Documentation (1-2 days)

**Optimization** (if time permits):
- Task 14: Performance Optimization (2-3 days)
- Task 15: Monitoring Dashboard (2 days)

**Deliverables**:
- Production-ready system
- Complete documentation
- Performance optimized

---

## üéØ Critical Path Tasks (Must Complete First)

1. **Task 8**: Validation Scripts ‚≠ê (1-2 days)
   - **Blocks**: Task 2
   - **Why Critical**: Required for validation loop

2. **Task 1**: LLM Generator ‚≠ê (3-4 days)
   - **Blocks**: Task 5
   - **Why Critical**: Core generation logic

3. **Task 3**: Prompt Builder ‚≠ê (2-3 days)
   - **Blocks**: Task 5
   - **Why Critical**: LLM needs good prompts

4. **Task 2**: Code Validator ‚≠ê (3-4 days)
   - **Blocks**: Task 5
   - **Why Critical**: Quality assurance

5. **Task 5**: Generator Service ‚≠ê (2-3 days)
   - **Blocks**: Tasks 9, 10, 11
   - **Why Critical**: Orchestrates everything

**Total Critical Path**: ~13-19 days

---

## üìä Team Allocation Recommendations

### Backend/AI Team (2-3 developers)
- **Developer 1**: Tasks 1, 5 (LLM generation, orchestration)
- **Developer 2**: Tasks 2, 8 (Validation, Node.js scripts)
- **Developer 3**: Tasks 3, 4, 6, 7 (Prompt building, simplification)

### Frontend Team (1 developer)
- **Developer 1**: Task 11 (UI updates)

### QA/Testing Team (1 developer)
- **Developer 1**: Tasks 9, E2E (Testing, integration)

### DevOps/Documentation (1 developer)
- **Developer 1**: Tasks 12, 13, 15 (Cleanup, docs, monitoring)

**Total Team**: 5-6 developers

---

## ‚úÖ Acceptance Gates

### Gate 1: Core Functionality (End of Sprint 2)
- [ ] LLM generates valid TypeScript components
- [ ] Validation loop fixes at least 80% of errors
- [ ] End-to-end generation works for Button pattern
- [ ] Quality score ‚â•7/10 on test cases

**Decision**: Proceed to testing phase or iterate?

---

### Gate 2: Quality & Coverage (End of Sprint 3)
- [ ] All patterns generate successfully
- [ ] Test coverage >90%
- [ ] Quality score ‚â•8/10 on average
- [ ] Performance: p50 <45s, p95 <75s (relaxed targets)

**Decision**: Proceed to production or optimize further?

---

### Gate 3: Production Ready (End of Sprint 4)
- [ ] All tests passing
- [ ] Documentation complete
- [ ] Frontend displays results correctly
- [ ] E2E tests passing
- [ ] Performance targets met (p50 <30s, p95 <60s)

**Decision**: Deploy to production or hold?

---

## üö® Risk Mitigation by Task Type

### Backend Risks
- **Risk**: LLM generates invalid code consistently
- **Mitigation**: Extensive prompt engineering (Task 3), validation loop (Task 2)
- **Contingency**: Fall back to hybrid approach (LLM + templates)

### Frontend Risks
- **Risk**: Breaking changes to UI
- **Mitigation**: Feature flag, backward compatibility
- **Contingency**: Revert to old UI, iterate separately

### Integration Risks
- **Risk**: Validation scripts don't work cross-platform
- **Mitigation**: Docker environment, graceful degradation
- **Contingency**: Skip validation, manual review

### Performance Risks
- **Risk**: LLM latency too high
- **Mitigation**: Parallel operations, caching (Task 14)
- **Contingency**: Use faster model (gpt-4o-mini), reduce prompt size

---

## üìà Success Metrics by Task Type

### Backend Metrics
- TypeScript compilation success: 60% ‚Üí 95%
- Generation latency p50: 60s ‚Üí 30s
- Fix success rate: N/A ‚Üí 80%+
- Token usage per request: <10k tokens

### Frontend Metrics
- UI load time: No regression
- User satisfaction: >8/10
- Error visibility: 100% (all errors shown)

### Integration Metrics
- Test coverage: 85% ‚Üí 90%+
- E2E test pass rate: 100%
- Build success rate: 100%

### Performance Metrics
- Cache hit rate: >40%
- Validation time: <5s
- Overall latency p95: <60s

---

## üìù Notes

### Parallel Work Opportunities
- **Week 1**: Task 1, 3, 8 can run in parallel (different developers)
- **Week 2**: Task 2, 4 can run in parallel
- **Week 3**: Task 6, 7, 9 can run in parallel
- **Week 4**: Task 11, 12, 13 can run in parallel

### Quick Wins
- Task 8 (Validation Scripts): Can be completed in 1-2 days, unblocks Task 2
- Task 6, 7 (Simplification): Easy deletions, quick wins
- Task 12 (Cleanup): Satisfying to remove old code

### High-Risk Tasks
- Task 2 (Validator): Complex, must handle many edge cases
- Task 5 (Generator Service): Orchestrates everything, many integration points
- Task 14 (Performance): May reveal unexpected bottlenecks

---

## üîó Cross-References

- **Main Epic**: `.claude/epics/04.5-llm-first-generation-refactor.md`
- **Current Implementation**: `.claude/epics/04-code-generation.md`
- **Related Epic**: `.claude/epics/05-quality-validation.md`

---

---

## üîÄ Commit Strategy

### Philosophy
- **Commit early, commit often** - Small, atomic commits for easy review and rollback
- **Feature flags** - Keep main branch deployable at all times
- **Semantic commits** - Follow conventional commit format
- **PR-based workflow** - All changes through pull requests with reviews

---

### Commit Conventions

#### Format
```
<type>(<scope>): <subject>

[optional body]

[optional footer]
```

#### Types for Epic 4.5
- `feat`: New feature (LLM generator, validator, etc.)
- `refactor`: Code refactoring (simplify parser, assembler)
- `test`: Adding or updating tests
- `perf`: Performance improvements
- `docs`: Documentation updates
- `chore`: Maintenance (cleanup old modules)
- `fix`: Bug fixes during implementation

#### Scopes
- `llm-gen`: LLM generator module
- `validator`: Code validator
- `prompt`: Prompt builder
- `exemplars`: Exemplar library
- `generator`: Generator service
- `parser`: Pattern parser
- `assembler`: Code assembler
- `api`: API endpoints
- `frontend`: Frontend components
- `tests`: Test files
- `scripts`: Validation scripts
- `docs`: Documentation

#### Examples
```bash
feat(llm-gen): add LLMComponentGenerator class with structured output

- Implement OpenAI structured outputs
- Add LangSmith tracing
- Include error handling with retries
- Track token usage

Refs: #Epic4.5, Task-1

feat(validator): implement validation loop with LLM fixes

- Add TypeScript validation via subprocess
- Add ESLint validation
- Implement LLM-based fix generator
- Add max 2 retry limit

Refs: #Epic4.5, Task-2

refactor(generator): simplify to 3-stage pipeline

BREAKING CHANGE: Removes 8-stage template-based pipeline

- Remove old stage methods (inject_tokens, etc.)
- Implement new LLM-first pipeline
- Update stage tracking to 3 stages
- Preserve LangSmith tracing

Refs: #Epic4.5, Task-5

chore(generation): delete 6 deprecated modules

- Remove token_injector.py
- Remove tailwind_generator.py
- Remove requirement_implementer.py
- Remove a11y_enhancer.py
- Remove type_generator.py
- Remove storybook_generator.py

Refs: #Epic4.5, Task-12
```

---

### Branch Strategy

#### Main Branches
- `main` - Production-ready code
- `develop` - Integration branch for Epic 4.5

#### Feature Branches
Create from `develop`, merge back to `develop`:

```bash
# Task-based branches
feature/epic4.5-task1-llm-generator
feature/epic4.5-task2-validator
feature/epic4.5-task3-prompt-builder
feature/epic4.5-task4-exemplars
feature/epic4.5-task5-generator-service
feature/epic4.5-task6-pattern-parser
feature/epic4.5-task7-code-assembler
feature/epic4.5-task8-validation-scripts
feature/epic4.5-task9-update-tests
feature/epic4.5-task10-api-endpoints
feature/epic4.5-task11-frontend
feature/epic4.5-task12-cleanup
feature/epic4.5-task13-documentation
feature/epic4.5-task14-performance
feature/epic4.5-task15-monitoring
```

#### Release Branch
After Sprint 4 completion:
```bash
release/epic4.5-llm-first-generation
```

---

### Commit Strategy by Task

#### Task 1: LLM Generator (3-4 commits)
```bash
# Commit 1: Foundation
feat(llm-gen): add LLMComponentGenerator class skeleton
- Add class structure with type hints
- Define output schema (LLMGeneratedCode)
- Add basic error handling

# Commit 2: Core Logic
feat(llm-gen): implement structured output generation
- Add OpenAI API integration
- Implement prompt construction
- Parse structured JSON response

# Commit 3: Observability
feat(llm-gen): add LangSmith tracing and token tracking
- Integrate LangSmith decorators
- Track token usage per request
- Add retry logic with exponential backoff

# Commit 4: Tests
test(llm-gen): add unit tests for LLM generator
- Test prompt construction
- Test structured output parsing
- Test error handling
- Mock OpenAI API calls
```

#### Task 2: Code Validator (4-5 commits)
```bash
# Commit 1: Foundation
feat(validator): add CodeValidator class structure
- Define validation result schema
- Add subprocess integration placeholders

# Commit 2: TypeScript Validation
feat(validator): implement TypeScript validation
- Add subprocess call to validate_typescript.js
- Parse validation results
- Handle errors gracefully

# Commit 3: ESLint Validation
feat(validator): implement ESLint validation
- Add subprocess call to validate_eslint.js
- Parse lint results
- Add parallel validation

# Commit 4: Fix Loop
feat(validator): add LLM-based fix loop
- Implement fix generator using LLM
- Add iterative fix logic (max 2 retries)
- Track fix attempts in metadata

# Commit 5: Tests
test(validator): add comprehensive validator tests
- Test TypeScript validation
- Test ESLint validation
- Test fix loop convergence
- Mock subprocess calls
```

#### Task 3: Prompt Builder (3 commits)
```bash
# Commit 1: Core Builder
feat(prompt): add PromptBuilder class with templates
- Create system prompt template
- Add prompt construction logic
- Format patterns, tokens, requirements

# Commit 2: Exemplars
feat(prompt): integrate exemplar selection
- Add exemplar loader integration
- Implement selection logic
- Format exemplars for prompts

# Commit 3: Optimization & Tests
feat(prompt): add token optimization and versioning
- Implement token counting
- Add prompt versioning support
- Add anti-pattern examples

test(prompt): add prompt builder tests
```

#### Task 4: Exemplar Library (2 commits)
```bash
# Commit 1: Library Structure
feat(exemplars): create exemplar library structure
- Add backend/data/exemplars/ directory
- Create 5 exemplar directories
- Add README for each exemplar

# Commit 2: Loader & Content
feat(exemplars): add ExemplarLoader and exemplar content
- Implement ExemplarLoader class
- Add hand-crafted exemplars (Button, Card, Input, etc.)
- Add metadata.json for each exemplar
- Add selection logic

test(exemplars): add exemplar loader tests
```

#### Task 5: Generator Service Refactor (5-6 commits)
```bash
# Commit 1: Add Feature Flag
feat(generator): add LLM_GENERATION feature flag
- Add ENABLE_LLM_GENERATION env var
- Add flag check in generator service

# Commit 2: New Pipeline Structure
refactor(generator): add new 3-stage pipeline structure
- Add new stage enum (GENERATING, VALIDATING, POST_PROCESSING)
- Keep old pipeline intact (behind flag)
- Add routing logic based on flag

# Commit 3: Integrate LLM Generator
feat(generator): integrate LLM generator in new pipeline
- Call LLMComponentGenerator
- Add error handling
- Track generation metrics

# Commit 4: Integrate Validator
feat(generator): integrate code validator with fix loop
- Call CodeValidator after generation
- Handle validation failures
- Track fix attempts

# Commit 5: Remove Old Pipeline
refactor(generator): remove old 8-stage pipeline

BREAKING CHANGE: Removes template-based generation stages

- Delete old stage method calls
- Remove unused imports
- Clean up dead code

# Commit 6: Tests
test(generator): rewrite generator service tests
- Test new 3-stage pipeline
- Test validation integration
- Test feature flag toggling
```

#### Task 6 & 7: Simplification (2 commits each)
```bash
# Task 6
refactor(parser): simplify pattern parser
- Remove modification point detection
- Remove regex-based code analysis
- Keep only basic parsing and metadata

test(parser): update pattern parser tests

# Task 7
refactor(assembler): simplify code assembler
- Remove multi-section assembly
- Keep only provenance, imports, formatting
- Delegate to specialized modules

test(assembler): update code assembler tests
```

#### Task 8: Validation Scripts (2 commits)
```bash
# Commit 1: TypeScript Script
feat(scripts): add TypeScript validation script
- Create validate_typescript.js
- Implement ts.createProgram validation
- Return JSON with errors/warnings
- Add cleanup logic

# Commit 2: ESLint Script
feat(scripts): add ESLint validation script
- Create validate_eslint.js
- Implement ESLint programmatic API
- Return JSON with errors/warnings
- Add timeout handling

test(scripts): add validation script tests
```

#### Task 9: Update Tests (Multiple commits)
```bash
test(generator): rewrite generator service tests
test(llm-gen): add LLM generator unit tests
test(validator): add code validator tests
test(prompt): add prompt builder tests
test(integration): add end-to-end integration tests
test(coverage): improve test coverage to >90%
```

#### Task 10: API Updates (2 commits)
```bash
# Commit 1: Schema Updates
feat(api): update generation response schema
- Add validation_results field
- Add quality_scores field
- Update OpenAPI docs

# Commit 2: Error Handling
feat(api): improve error handling and messages
- Add detailed validation errors
- Add streaming support (optional)
- Add feature flag support

test(api): update API endpoint tests
```

#### Task 11: Frontend (3-4 commits)
```bash
# Commit 1: Type Updates
feat(frontend): update generation types
- Update GenerationResponse types
- Add validation result types
- Add quality score types

# Commit 2: Progress Updates
feat(frontend): update progress to 3 stages
- Update GenerationProgress component
- Show new stage names
- Update progress indicators

# Commit 3: Quality Display
feat(frontend): add validation results and quality scores UI
- Show TypeScript/ESLint status
- Display quality scores
- Show fix attempts indicator

# Commit 4: Error Handling
feat(frontend): improve error message display
- Show validation errors with line numbers
- Add regeneration suggestions
- Update error styling

test(frontend): add frontend component tests
```

#### Task 12: Cleanup (1 commit)
```bash
chore(generation): delete deprecated modules and tests

Removes 12 files:
- 6 backend modules (token_injector, tailwind_generator, etc.)
- 6 test files

Updates:
- Remove imports from generator_service.py
- Update types.py with new schemas
- Update README.md

Refs: #Epic4.5, Task-12
```

#### Task 13: Documentation (3-4 commits)
```bash
docs(generation): update README with new architecture
docs(generation): add PROMPTING_GUIDE.md
docs(generation): add TROUBLESHOOTING.md
docs(epic): mark Epic 4 complete, update metrics
```

#### Task 14: Performance (2-3 commits)
```bash
perf(validator): add parallel validation
perf(llm-gen): implement prompt caching
perf(generator): add request caching with Redis
perf(llm-gen): optimize LLM parameters

test(perf): add performance benchmarks
```

#### Task 15: Monitoring (2 commits)
```bash
feat(monitoring): add generation quality metrics
- Track success rates
- Track quality scores
- Add Prometheus metrics

feat(monitoring): add LangSmith metadata enrichment
- Add quality scores to traces
- Add custom metadata
- Enable filtering by quality

docs(monitoring): add monitoring dashboard setup
```

---

### Pull Request Strategy

#### PR Size Guidelines
- **Small PRs**: 1 task = 1 PR (preferred)
- **Max size**: 500 lines changed (excluding tests)
- **Exception**: Task 5 and Task 9 may need larger PRs

#### PR Template
```markdown
## Epic 4.5: Task N - [Task Name]

### Description
[Brief description of changes]

### Changes
- [ ] Added X
- [ ] Updated Y
- [ ] Removed Z

### Testing
- [ ] Unit tests added/updated
- [ ] Integration tests passing
- [ ] Manual testing completed

### Checklist
- [ ] Code follows style guide
- [ ] Tests pass locally
- [ ] Documentation updated
- [ ] No breaking changes (or documented)
- [ ] Feature flag used (if applicable)

### Related
- Epic: #Epic4.5
- Task: Task-N
- Depends on: #PR-XXX (if applicable)

### Screenshots (if applicable)
[Add screenshots for frontend changes]
```

#### PR Review Guidelines
- **Require 2 approvals** for critical path tasks (1, 2, 5)
- **Require 1 approval** for other tasks
- **Auto-merge** after approvals + CI pass
- **Block on**:
  - Test failures
  - Coverage drop >2%
  - Breaking changes without migration plan

---

### Sprint Merge Strategy

#### Sprint 1 (End of Week 2)
```bash
# Merge to develop after each PR approved
feature/epic4.5-task8-validation-scripts ‚Üí develop
feature/epic4.5-task1-llm-generator ‚Üí develop
feature/epic4.5-task3-prompt-builder ‚Üí develop

# Sprint 1 checkpoint
git checkout develop
git tag sprint1-checkpoint
```

#### Sprint 2 (End of Week 2)
```bash
feature/epic4.5-task2-validator ‚Üí develop
feature/epic4.5-task5-generator-service ‚Üí develop
feature/epic4.5-task4-exemplars ‚Üí develop

git checkout develop
git tag sprint2-checkpoint
```

#### Sprint 3 (End of Week 3)
```bash
feature/epic4.5-task6-pattern-parser ‚Üí develop
feature/epic4.5-task7-code-assembler ‚Üí develop
feature/epic4.5-task9-update-tests ‚Üí develop
feature/epic4.5-task10-api-endpoints ‚Üí develop

git checkout develop
git tag sprint3-checkpoint
```

#### Sprint 4 (End of Week 4)
```bash
feature/epic4.5-task11-frontend ‚Üí develop
feature/epic4.5-task12-cleanup ‚Üí develop
feature/epic4.5-task13-documentation ‚Üí develop

# Optional (if time)
feature/epic4.5-task14-performance ‚Üí develop
feature/epic4.5-task15-monitoring ‚Üí develop

git checkout develop
git tag sprint4-checkpoint
```

#### Production Release
```bash
# Create release branch
git checkout develop
git checkout -b release/epic4.5-llm-first-generation

# Final testing, bug fixes on release branch
# Update version, changelog

# Merge to main
git checkout main
git merge release/epic4.5-llm-first-generation
git tag v2.0.0-epic4.5

# Merge back to develop
git checkout develop
git merge main
```

---

### Rollback Strategy

#### If Task Breaks Build
```bash
# Revert specific PR
git revert <commit-hash>

# Or revert to checkpoint
git reset --hard sprint2-checkpoint
```

#### If Feature Flag Needed
```bash
# Disable via environment variable
ENABLE_LLM_GENERATION=false

# No code rollback needed
```

#### If Production Issues
```bash
# Hotfix branch from main
git checkout main
git checkout -b hotfix/epic4.5-rollback

# Disable feature flag in production
# Or revert specific commits

git checkout main
git merge hotfix/epic4.5-rollback
git tag v2.0.1-hotfix
```

---

### Commit Metrics & Goals

#### Quality Metrics
- **Build success rate**: 100% (all commits pass CI)
- **Test coverage**: No commit reduces coverage by >2%
- **Review time**: <24 hours for small PRs, <48 hours for large PRs
- **Commit frequency**: 2-3 commits per developer per day

#### Velocity Tracking
- **Sprint 1**: ~8-12 commits
- **Sprint 2**: ~12-16 commits
- **Sprint 3**: ~15-20 commits
- **Sprint 4**: ~10-15 commits
- **Total**: ~50-60 commits

---

### Continuous Integration Checks

#### On Every Commit
```bash
# Backend checks
pytest backend/tests/ -v --cov=backend/src --cov-fail-under=90
black --check backend/
isort --check backend/
mypy backend/src/

# Frontend checks (if applicable)
cd app && npm test
cd app && npm run lint
cd app && npm run type-check

# Integration checks
# (Run on feature branch, not every commit)
```

#### On PR Creation
- All commit checks +
- E2E tests (if Task 11 merged)
- Performance benchmarks (if Task 14 merged)
- Security scan

#### On Merge to Develop
- Full test suite
- Integration tests
- Deploy to staging environment
- Smoke tests

---

### Git Hooks (Recommended)

#### Pre-commit Hook
```bash
#!/bin/bash
# .git/hooks/pre-commit

# Run linting
black backend/
isort backend/

# Run type checking
mypy backend/src/

# Stage formatted files
git add -u

# Prevent commit if tests fail (optional)
# pytest backend/tests/ -v -x
```

#### Commit-msg Hook
```bash
#!/bin/bash
# .git/hooks/commit-msg

# Enforce conventional commit format
commit_msg_file=$1
commit_msg=$(cat "$commit_msg_file")

pattern="^(feat|fix|docs|style|refactor|perf|test|chore)(\([a-z-]+\))?: .{1,72}"

if ! echo "$commit_msg" | grep -qE "$pattern"; then
    echo "ERROR: Commit message must follow conventional commit format"
    echo "Format: <type>(<scope>): <subject>"
    echo "Example: feat(llm-gen): add structured output parsing"
    exit 1
fi
```

---

### Troubleshooting Commits

#### Commit Too Large
```bash
# Split into smaller commits
git reset HEAD~1
git add -p  # Stage hunks interactively
git commit -m "feat(scope): first part"
git add -p
git commit -m "feat(scope): second part"
```

#### Wrong Commit Message
```bash
# Amend last commit
git commit --amend -m "feat(correct-scope): correct message"

# Interactive rebase for older commits
git rebase -i HEAD~3
# Mark commit as 'reword', save, update message
```

#### Need to Squash Commits
```bash
# Squash last 3 commits
git rebase -i HEAD~3
# Mark commits as 'squash', save, write combined message
```

---

### Success Criteria for Commit Strategy

- [ ] All commits follow conventional commit format
- [ ] All commits pass CI checks
- [ ] Average PR size <300 lines (excluding tests)
- [ ] Average PR review time <36 hours
- [ ] Zero force pushes to main/develop
- [ ] 100% commit traceability (every commit linked to task)
- [ ] All breaking changes documented in commit messages

---

**Last Updated**: 2025-10-07
**Version**: 1.0
**Owner**: Backend/AI Team
