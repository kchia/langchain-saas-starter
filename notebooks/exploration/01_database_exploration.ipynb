{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Database Exploration and Analysis\n",
    "\n",
    "This notebook provides comprehensive exploration of the Component Forge database, including schema analysis, data quality assessment, and relationship mapping.\n",
    "\n",
    "## What You'll Explore\n",
    "\n",
    "1. Database schema and table relationships\n",
    "2. Data distribution and quality metrics\n",
    "3. User activity patterns\n",
    "4. Content analysis and categorization\n",
    "5. Performance insights and optimization opportunities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Database Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import asyncio\n",
    "from pathlib import Path\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Add project paths\n",
    "project_root = Path.cwd().parent if Path.cwd().name == 'notebooks' else Path.cwd()\n",
    "backend_src = project_root / 'backend' / 'src'\n",
    "sys.path.insert(0, str(backend_src))\n",
    "\n",
    "# Load environment\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(project_root / 'backend' / '.env')\n",
    "\n",
    "# Data science imports\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "# Project imports\n",
    "from core.database import database_health_check\n",
    "from notebooks.utils.database_helpers import (\n",
    "    sync_get_database_stats,\n",
    "    sync_execute_query,\n",
    "    sync_get_users_df,\n",
    "    sync_get_documents_df,\n",
    "    sync_get_conversations_df,\n",
    "    sync_get_messages_df\n",
    ")\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "print(\"üîç Database exploration environment ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Database Health and Schema Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check database health\n",
    "health = await database_health_check()\n",
    "\n",
    "print(\"üè• Database Health Check:\")\n",
    "print(f\"  Status: {health['status']}\")\n",
    "if health['status'] == 'healthy':\n",
    "    print(f\"  Database Version: {health.get('database_version', 'Unknown')}\")\n",
    "    print(f\"  Tables: {health.get('table_count', 'Unknown')}\")\n",
    "    print(f\"  Connection Time: {health.get('connection_time_ms', 'Unknown')}ms\")\n",
    "    print(f\"  Pool Size: {health.get('pool_size', 'Unknown')}\")\n",
    "    print(f\"  Active Connections: {health.get('checked_out_connections', 'Unknown')}\")\n",
    "\n",
    "# Get basic statistics\n",
    "stats = sync_get_database_stats()\n",
    "print(f\"\\nüìä Record Counts:\")\n",
    "for table, count in stats.items():\n",
    "    print(f\"  {table}: {count:,}\")\n",
    "\n",
    "# Visualize data distribution\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Bar chart of record counts\n",
    "tables = list(stats.keys())\n",
    "counts = list(stats.values())\n",
    "colors = sns.color_palette(\"husl\", len(tables))\n",
    "\n",
    "bars = ax1.bar(tables, counts, color=colors)\n",
    "ax1.set_title('Database Record Distribution')\n",
    "ax1.set_ylabel('Number of Records')\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Add value labels\n",
    "for bar, count in zip(bars, counts):\n",
    "    height = bar.get_height()\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2., height + max(counts)*0.01,\n",
    "             f'{count}', ha='center', va='bottom')\n",
    "\n",
    "# Pie chart for proportions\n",
    "if sum(counts) > 0:\n",
    "    ax2.pie(counts, labels=tables, autopct='%1.1f%%', colors=colors)\n",
    "    ax2.set_title('Data Distribution by Table')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Schema Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get table schema information\n",
    "schema_query = \"\"\"\n",
    "SELECT \n",
    "    table_name,\n",
    "    column_name,\n",
    "    data_type,\n",
    "    is_nullable,\n",
    "    column_default\n",
    "FROM information_schema.columns \n",
    "WHERE table_schema = 'public' \n",
    "AND table_name IN ('users', 'documents', 'conversations', 'messages', 'document_chunks', 'embedding_models', 'evaluation_runs')\n",
    "ORDER BY table_name, ordinal_position\n",
    "\"\"\"\n",
    "\n",
    "schema_df = sync_execute_query(schema_query)\n",
    "\n",
    "print(\"üóÇÔ∏è Database Schema Overview:\")\n",
    "for table in schema_df['table_name'].unique():\n",
    "    table_schema = schema_df[schema_df['table_name'] == table]\n",
    "    print(f\"\\nüìã Table: {table}\")\n",
    "    print(f\"  Columns: {len(table_schema)}\")\n",
    "    \n",
    "    # Show key columns\n",
    "    key_columns = table_schema[['column_name', 'data_type', 'is_nullable']].head()\n",
    "    for _, col in key_columns.iterrows():\n",
    "        nullable = \"NULL\" if col['is_nullable'] == 'YES' else \"NOT NULL\"\n",
    "        print(f\"    {col['column_name']}: {col['data_type']} ({nullable})\")\n",
    "    \n",
    "    if len(table_schema) > 5:\n",
    "        print(f\"    ... and {len(table_schema) - 5} more columns\")\n",
    "\n",
    "# Analyze data types distribution\n",
    "dtype_counts = schema_df['data_type'].value_counts()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "bars = ax.bar(dtype_counts.index[:10], dtype_counts.values[:10])  # Top 10 data types\n",
    "ax.set_title('Most Common Data Types')\n",
    "ax.set_ylabel('Number of Columns')\n",
    "ax.tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Detailed Table Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Users Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and analyze users\n",
    "users_df = sync_get_users_df()\n",
    "\n",
    "if len(users_df) > 0:\n",
    "    print(f\"üë• Users Analysis ({len(users_df)} users):\")\n",
    "    \n",
    "    # Basic statistics\n",
    "    active_users = users_df['is_active'].sum()\n",
    "    admin_users = users_df['is_admin'].sum()\n",
    "    total_logins = users_df['login_count'].sum()\n",
    "    \n",
    "    print(f\"  Active users: {active_users}/{len(users_df)} ({active_users/len(users_df)*100:.1f}%)\")\n",
    "    print(f\"  Admin users: {admin_users}/{len(users_df)} ({admin_users/len(users_df)*100:.1f}%)\")\n",
    "    print(f\"  Total logins: {total_logins:,}\")\n",
    "    print(f\"  Average logins per user: {total_logins/len(users_df):.1f}\")\n",
    "    \n",
    "    # User activity distribution\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Login count distribution\n",
    "    ax1.hist(users_df['login_count'], bins=10, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "    ax1.set_title('Login Count Distribution')\n",
    "    ax1.set_xlabel('Login Count')\n",
    "    ax1.set_ylabel('Number of Users')\n",
    "    \n",
    "    # User types\n",
    "    user_types = users_df.groupby(['is_admin', 'is_active']).size().reset_index(name='count')\n",
    "    user_types['label'] = user_types.apply(\n",
    "        lambda x: f\"{'Admin' if x['is_admin'] else 'User'} ({'Active' if x['is_active'] else 'Inactive'})\", \n",
    "        axis=1\n",
    "    )\n",
    "    ax2.pie(user_types['count'], labels=user_types['label'], autopct='%1.0f%%')\n",
    "    ax2.set_title('User Type Distribution')\n",
    "    \n",
    "    # Registration timeline (if created_at is available)\n",
    "    if 'created_at' in users_df.columns:\n",
    "        users_df['created_date'] = pd.to_datetime(users_df['created_at']).dt.date\n",
    "        daily_registrations = users_df.groupby('created_date').size()\n",
    "        ax3.plot(daily_registrations.index, daily_registrations.values, marker='o')\n",
    "        ax3.set_title('User Registration Timeline')\n",
    "        ax3.set_xlabel('Date')\n",
    "        ax3.set_ylabel('New Registrations')\n",
    "        ax3.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Last login analysis (if available)\n",
    "    if 'last_login_at' in users_df.columns and users_df['last_login_at'].notna().any():\n",
    "        users_df['last_login_date'] = pd.to_datetime(users_df['last_login_at']).dt.date\n",
    "        recent_logins = users_df['last_login_date'].value_counts().sort_index()\n",
    "        ax4.bar(range(len(recent_logins)), recent_logins.values)\n",
    "        ax4.set_title('Recent Login Activity')\n",
    "        ax4.set_xlabel('Date')\n",
    "        ax4.set_ylabel('Users Logged In')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Display user details\n",
    "    print(\"\\nüë§ User Details:\")\n",
    "    display(users_df[['username', 'email', 'is_active', 'is_admin', 'login_count']].head())\nelse:\n",
    "    print(\"No users found in database\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Documents Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and analyze documents\n",
    "docs_df = sync_get_documents_df()\n",
    "\n",
    "if len(docs_df) > 0:\n",
    "    print(f\"üìÑ Documents Analysis ({len(docs_df)} documents):\")\n",
    "    \n",
    "    # Basic statistics\n",
    "    total_words = docs_df['word_count'].sum()\n",
    "    avg_words = docs_df['word_count'].mean()\n",
    "    total_size = docs_df['file_size'].sum()\n",
    "    \n",
    "    print(f\"  Total words: {total_words:,}\")\n",
    "    print(f\"  Average words per document: {avg_words:.0f}\")\n",
    "    print(f\"  Total file size: {total_size/1024:.1f} KB\")\n",
    "    print(f\"  Content types: {docs_df['content_type'].nunique()}\")\n",
    "    print(f\"  Categories: {docs_df['category'].nunique()}\")\n",
    "    \n",
    "    # Document metrics analysis\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Word count distribution\n",
    "    ax1.hist(docs_df['word_count'], bins=15, alpha=0.7, color='lightgreen', edgecolor='black')\n",
    "    ax1.axvline(avg_words, color='red', linestyle='--', label=f'Mean: {avg_words:.0f}')\n",
    "    ax1.set_title('Document Length Distribution')\n",
    "    ax1.set_xlabel('Word Count')\n",
    "    ax1.set_ylabel('Number of Documents')\n",
    "    ax1.legend()\n",
    "    \n",
    "    # Content types\n",
    "    content_counts = docs_df['content_type'].value_counts()\n",
    "    ax2.pie(content_counts.values, labels=content_counts.index, autopct='%1.0f%%')\n",
    "    ax2.set_title('Content Type Distribution')\n",
    "    \n",
    "    # Categories\n",
    "    category_counts = docs_df['category'].value_counts()\n",
    "    ax3.bar(category_counts.index, category_counts.values)\n",
    "    ax3.set_title('Document Categories')\n",
    "    ax3.set_xlabel('Category')\n",
    "    ax3.set_ylabel('Number of Documents')\n",
    "    ax3.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # File size vs word count correlation\n",
    "    docs_df['file_size_kb'] = docs_df['file_size'] / 1024\n",
    "    ax4.scatter(docs_df['word_count'], docs_df['file_size_kb'], alpha=0.7)\n",
    "    ax4.set_title('Word Count vs File Size')\n",
    "    ax4.set_xlabel('Word Count')\n",
    "    ax4.set_ylabel('File Size (KB)')\n",
    "    \n",
    "    # Add correlation coefficient\n",
    "    correlation = docs_df['word_count'].corr(docs_df['file_size_kb'])\n",
    "    ax4.text(0.05, 0.95, f'Correlation: {correlation:.3f}', \n",
    "             transform=ax4.transAxes, verticalalignment='top',\n",
    "             bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Processing status analysis\n",
    "    status_counts = docs_df['processing_status'].value_counts()\n",
    "    print(f\"\\n‚öôÔ∏è Processing Status:\")\n",
    "    for status, count in status_counts.items():\n",
    "        print(f\"  {status}: {count} documents ({count/len(docs_df)*100:.1f}%)\")\n",
    "    \n",
    "    # Category analysis\n",
    "    print(f\"\\nüìÇ Category Analysis:\")\n",
    "    category_stats = docs_df.groupby('category').agg({\n",
    "        'word_count': ['count', 'sum', 'mean'],\n",
    "        'file_size': 'sum'\n",
    "    }).round(2)\n",
    "    \n",
    "    display(category_stats)\nelse:\n",
    "    print(\"No documents found in database\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Conversations and Messages Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and analyze conversations\n",
    "conversations_df = sync_get_conversations_df()\n",
    "messages_df = sync_get_messages_df()\n",
    "\n",
    "if len(conversations_df) > 0:\n",
    "    print(f\"üí¨ Conversations Analysis ({len(conversations_df)} conversations):\")\n",
    "    \n",
    "    # Basic statistics\n",
    "    total_messages = messages_df.shape[0] if len(messages_df) > 0 else 0\n",
    "    total_tokens = conversations_df['total_tokens_used'].sum()\n",
    "    avg_messages = conversations_df['message_count'].mean()\n",
    "    unique_users = conversations_df['user_id'].nunique()\n",
    "    \n",
    "    print(f\"  Total messages: {total_messages:,}\")\n",
    "    print(f\"  Total tokens used: {total_tokens:,}\")\n",
    "    print(f\"  Average messages per conversation: {avg_messages:.1f}\")\n",
    "    print(f\"  Active users: {unique_users}\")\n",
    "    \n",
    "    # Conversation analysis\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Message count distribution\n",
    "    ax1.hist(conversations_df['message_count'], bins=10, alpha=0.7, color='orange', edgecolor='black')\n",
    "    ax1.axvline(avg_messages, color='red', linestyle='--', label=f'Mean: {avg_messages:.1f}')\n",
    "    ax1.set_title('Messages per Conversation')\n",
    "    ax1.set_xlabel('Message Count')\n",
    "    ax1.set_ylabel('Number of Conversations')\n",
    "    ax1.legend()\n",
    "    \n",
    "    # Token usage distribution\n",
    "    ax2.hist(conversations_df['total_tokens_used'], bins=10, alpha=0.7, color='purple', edgecolor='black')\n",
    "    ax2.set_title('Token Usage Distribution')\n",
    "    ax2.set_xlabel('Total Tokens Used')\n",
    "    ax2.set_ylabel('Number of Conversations')\n",
    "    \n",
    "    # Model usage\n",
    "    model_counts = conversations_df['model_name'].value_counts()\n",
    "    ax3.pie(model_counts.values, labels=model_counts.index, autopct='%1.0f%%')\n",
    "    ax3.set_title('Model Usage Distribution')\n",
    "    \n",
    "    # Conversations by user\n",
    "    user_conv_counts = conversations_df['username'].value_counts()\n",
    "    ax4.bar(user_conv_counts.index, user_conv_counts.values)\n",
    "    ax4.set_title('Conversations by User')\n",
    "    ax4.set_xlabel('Username')\n",
    "    ax4.set_ylabel('Number of Conversations')\n",
    "    ax4.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Message role analysis\n",
    "    if len(messages_df) > 0:\n",
    "        print(f\"\\nüó®Ô∏è Message Analysis:\")\n",
    "        role_counts = messages_df['role'].value_counts()\n",
    "        for role, count in role_counts.items():\n",
    "            print(f\"  {role}: {count:,} messages ({count/len(messages_df)*100:.1f}%)\")\n",
    "        \n",
    "        # Token usage by role (if available)\n",
    "        if 'total_tokens' in messages_df.columns:\n",
    "            token_by_role = messages_df.groupby('role')['total_tokens'].agg(['sum', 'mean', 'count']).fillna(0)\n",
    "            print(f\"\\nüéØ Token Usage by Role:\")\n",
    "            display(token_by_role)\n",
    "    \n",
    "    # Temperature analysis\n",
    "    print(f\"\\nüå°Ô∏è Temperature Settings:\")\n",
    "    temp_stats = conversations_df['temperature'].describe()\n",
    "    for stat, value in temp_stats.items():\n",
    "        print(f\"  {stat}: {value:.3f}\")\nelse:\n",
    "    print(\"No conversations found in database\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Cross-Table Relationship Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze relationships between tables\n",
    "relationship_query = \"\"\"\n",
    "SELECT \n",
    "    u.username,\n",
    "    u.is_admin,\n",
    "    u.login_count,\n",
    "    COUNT(DISTINCT c.id) as conversation_count,\n",
    "    COUNT(DISTINCT d.id) as document_count,\n",
    "    COUNT(m.id) as message_count,\n",
    "    SUM(CASE WHEN m.role = 'user' THEN 1 ELSE 0 END) as user_messages,\n",
    "    SUM(CASE WHEN m.role = 'assistant' THEN 1 ELSE 0 END) as assistant_messages,\n",
    "    SUM(m.total_tokens) as total_tokens,\n",
    "    AVG(m.total_tokens) as avg_tokens_per_message,\n",
    "    MAX(c.last_message_at) as last_activity\n",
    "FROM users u\n",
    "LEFT JOIN conversations c ON u.id = c.user_id\n",
    "LEFT JOIN documents d ON u.id = d.uploaded_by_id\n",
    "LEFT JOIN messages m ON c.id = m.conversation_id\n",
    "GROUP BY u.id, u.username, u.is_admin, u.login_count\n",
    "ORDER BY conversation_count DESC, total_tokens DESC\n",
    "\"\"\"\n",
    "\n",
    "user_activity_df = sync_execute_query(relationship_query)\n",
    "\n",
    "if len(user_activity_df) > 0:\n",
    "    print(\"üîó User Activity Correlation Analysis:\")\n",
    "    \n",
    "    # Display user activity summary\n",
    "    display(user_activity_df[[\n",
    "        'username', 'conversation_count', 'message_count', \n",
    "        'document_count', 'total_tokens', 'last_activity'\n",
    "    ]].head())\n",
    "    \n",
    "    # Correlation analysis\n",
    "    numeric_cols = ['login_count', 'conversation_count', 'message_count', 'total_tokens']\n",
    "    correlation_matrix = user_activity_df[numeric_cols].corr()\n",
    "    \n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Correlation heatmap\n",
    "    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, ax=ax1)\n",
    "    ax1.set_title('User Activity Correlations')\n",
    "    \n",
    "    # Login count vs conversations\n",
    "    ax2.scatter(user_activity_df['login_count'], user_activity_df['conversation_count'], alpha=0.7)\n",
    "    ax2.set_xlabel('Login Count')\n",
    "    ax2.set_ylabel('Conversation Count')\n",
    "    ax2.set_title('Logins vs Conversations')\n",
    "    \n",
    "    # Messages vs tokens\n",
    "    valid_data = user_activity_df.dropna(subset=['message_count', 'total_tokens'])\n",
    "    if len(valid_data) > 0:\n",
    "        ax3.scatter(valid_data['message_count'], valid_data['total_tokens'], alpha=0.7)\n",
    "        ax3.set_xlabel('Message Count')\n",
    "        ax3.set_ylabel('Total Tokens')\n",
    "        ax3.set_title('Messages vs Token Usage')\n",
    "    \n",
    "    # User type analysis\n",
    "    admin_stats = user_activity_df.groupby('is_admin')[numeric_cols].mean()\n",
    "    admin_stats.plot(kind='bar', ax=ax4)\n",
    "    ax4.set_title('Activity by User Type')\n",
    "    ax4.set_xlabel('Is Admin')\n",
    "    ax4.set_ylabel('Average Count')\n",
    "    ax4.tick_params(axis='x', rotation=0)\n",
    "    ax4.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Statistical insights\n",
    "    print(f\"\\nüìä Key Insights:\")\n",
    "    \n",
    "    # Most active users\n",
    "    most_active = user_activity_df.nlargest(3, 'conversation_count')\n",
    "    print(f\"  Most active users (by conversations):\")\n",
    "    for _, user in most_active.iterrows():\n",
    "        print(f\"    {user['username']}: {user['conversation_count']} conversations, {user['message_count']} messages\")\n",
    "    \n",
    "    # Token usage leaders\n",
    "    if user_activity_df['total_tokens'].notna().any():\n",
    "        token_leaders = user_activity_df.nlargest(3, 'total_tokens')\n",
    "        print(f\"  Top token users:\")\n",
    "        for _, user in token_leaders.iterrows():\n",
    "            tokens = user['total_tokens'] or 0\n",
    "            print(f\"    {user['username']}: {tokens:,.0f} tokens\")\n",
    "    \n",
    "    # Activity patterns\n",
    "    active_users = user_activity_df[user_activity_df['conversation_count'] > 0]\n",
    "    if len(active_users) > 0:\n",
    "        avg_conversations = active_users['conversation_count'].mean()\n",
    "        avg_messages = active_users['message_count'].mean()\n",
    "        print(f\"  Average conversations per active user: {avg_conversations:.1f}\")\n",
    "        print(f\"  Average messages per active user: {avg_messages:.1f}\")\nelse:\n",
    "    print(\"No user activity data available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Data Quality Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data quality checks\n",
    "print(\"üîç Data Quality Assessment:\")\n",
    "\n",
    "quality_issues = []\n",
    "\n",
    "# Check users table\n",
    "if len(users_df) > 0:\n",
    "    null_emails = users_df['email'].isnull().sum()\n",
    "    duplicate_emails = users_df['email'].duplicated().sum()\n",
    "    null_usernames = users_df['username'].isnull().sum()\n",
    "    duplicate_usernames = users_df['username'].duplicated().sum()\n",
    "    \n",
    "    print(f\"\\nüë• Users Quality:\")\n",
    "    print(f\"  Null emails: {null_emails}\")\n",
    "    print(f\"  Duplicate emails: {duplicate_emails}\")\n",
    "    print(f\"  Null usernames: {null_usernames}\")\n",
    "    print(f\"  Duplicate usernames: {duplicate_usernames}\")\n",
    "    \n",
    "    if null_emails > 0: quality_issues.append(f\"Users: {null_emails} null emails\")\n",
    "    if duplicate_emails > 0: quality_issues.append(f\"Users: {duplicate_emails} duplicate emails\")\n",
    "    if null_usernames > 0: quality_issues.append(f\"Users: {null_usernames} null usernames\")\n",
    "    if duplicate_usernames > 0: quality_issues.append(f\"Users: {duplicate_usernames} duplicate usernames\")\n",
    "\n",
    "# Check documents table\n",
    "if len(docs_df) > 0:\n",
    "    null_titles = docs_df['title'].isnull().sum()\n",
    "    zero_word_count = (docs_df['word_count'] == 0).sum()\n",
    "    negative_file_size = (docs_df['file_size'] < 0).sum()\n",
    "    failed_processing = (docs_df['processing_status'] == 'failed').sum()\n",
    "    \n",
    "    print(f\"\\nüìÑ Documents Quality:\")\n",
    "    print(f\"  Null titles: {null_titles}\")\n",
    "    print(f\"  Zero word count: {zero_word_count}\")\n",
    "    print(f\"  Negative file size: {negative_file_size}\")\n",
    "    print(f\"  Failed processing: {failed_processing}\")\n",
    "    \n",
    "    if null_titles > 0: quality_issues.append(f\"Documents: {null_titles} null titles\")\n",
    "    if zero_word_count > 0: quality_issues.append(f\"Documents: {zero_word_count} zero word count\")\n",
    "    if negative_file_size > 0: quality_issues.append(f\"Documents: {negative_file_size} negative file sizes\")\n",
    "    if failed_processing > 0: quality_issues.append(f\"Documents: {failed_processing} failed processing\")\n",
    "\n",
    "# Check conversations table\n",
    "if len(conversations_df) > 0:\n",
    "    null_titles = conversations_df['title'].isnull().sum()\n",
    "    zero_messages = (conversations_df['message_count'] == 0).sum()\n",
    "    negative_tokens = (conversations_df['total_tokens_used'] < 0).sum()\n",
    "    \n",
    "    print(f\"\\nüí¨ Conversations Quality:\")\n",
    "    print(f\"  Null titles: {null_titles}\")\n",
    "    print(f\"  Zero message count: {zero_messages}\")\n",
    "    print(f\"  Negative token count: {negative_tokens}\")\n",
    "    \n",
    "    if null_titles > 0: quality_issues.append(f\"Conversations: {null_titles} null titles\")\n",
    "    if zero_messages > 0: quality_issues.append(f\"Conversations: {zero_messages} zero message count\")\n",
    "    if negative_tokens > 0: quality_issues.append(f\"Conversations: {negative_tokens} negative tokens\")\n",
    "\n",
    "# Check messages table\n",
    "if len(messages_df) > 0:\n",
    "    null_content = messages_df['content_length'].isnull().sum() if 'content_length' in messages_df.columns else 0\n",
    "    invalid_roles = (~messages_df['role'].isin(['user', 'assistant', 'system'])).sum()\n",
    "    \n",
    "    print(f\"\\nüó®Ô∏è Messages Quality:\")\n",
    "    print(f\"  Invalid roles: {invalid_roles}\")\n",
    "    \n",
    "    if invalid_roles > 0: quality_issues.append(f\"Messages: {invalid_roles} invalid roles\")\n",
    "\n",
    "# Summary\n",
    "print(f\"\\nüìã Quality Summary:\")\n",
    "if quality_issues:\n",
    "    print(f\"  ‚ö†Ô∏è Issues found: {len(quality_issues)}\")\n",
    "    for issue in quality_issues:\n",
    "        print(f\"    - {issue}\")\nelse:\n",
    "    print(f\"  ‚úÖ No major quality issues detected\")\n",
    "\n",
    "# Data completeness analysis\n",
    "completeness_data = []\n",
    "\n",
    "for df_name, df in [('users', users_df), ('documents', docs_df), ('conversations', conversations_df)]:\n",
    "    if len(df) > 0:\n",
    "        completeness = {}\n",
    "        for col in df.columns:\n",
    "            if col in df.select_dtypes(include=[np.number]).columns:\n",
    "                non_null_pct = (df[col].notna().sum() / len(df)) * 100\n",
    "                completeness[col] = non_null_pct\n",
    "        \n",
    "        if completeness:\n",
    "            completeness_data.append({\n",
    "                'table': df_name,\n",
    "                'avg_completeness': np.mean(list(completeness.values())),\n",
    "                'min_completeness': min(completeness.values()),\n",
    "                'columns_checked': len(completeness)\n",
    "            })\n",
    "\n",
    "if completeness_data:\n",
    "    completeness_df = pd.DataFrame(completeness_data)\n",
    "    print(f\"\\nüìä Data Completeness (Numeric Columns):\")\n",
    "    display(completeness_df)\n",
    "\n",
    "    # Visualize completeness\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    bars = ax.bar(completeness_df['table'], completeness_df['avg_completeness'])\n",
    "    ax.set_title('Data Completeness by Table')\n",
    "    ax.set_ylabel('Average Completeness (%)')\n",
    "    ax.set_ylim(0, 100)\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, value in zip(bars, completeness_df['avg_completeness']):\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height + 1,\n",
    "                f'{value:.1f}%', ha='center', va='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Performance Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance analysis\n",
    "print(\"‚ö° Performance Insights:\")\n",
    "\n",
    "# Table size analysis\n",
    "table_size_query = \"\"\"\n",
    "SELECT \n",
    "    schemaname,\n",
    "    tablename,\n",
    "    attname as column_name,\n",
    "    n_distinct,\n",
    "    correlation\n",
    "FROM pg_stats \n",
    "WHERE schemaname = 'public' \n",
    "AND tablename IN ('users', 'documents', 'conversations', 'messages')\n",
    "ORDER BY tablename, attname\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    stats_df = sync_execute_query(table_size_query)\n",
    "    \n",
    "    if len(stats_df) > 0:\n",
    "        print(f\"\\nüìà Column Statistics:\")\n",
    "        \n",
    "        # Show high-cardinality columns (potential indexing candidates)\n",
    "        high_cardinality = stats_df[stats_df['n_distinct'] > 100].sort_values('n_distinct', ascending=False)\n",
    "        if len(high_cardinality) > 0:\n",
    "            print(f\"  High-cardinality columns (good for indexing):\")\n",
    "            for _, row in high_cardinality.head().iterrows():\n",
    "                print(f\"    {row['tablename']}.{row['column_name']}: {row['n_distinct']} distinct values\")\n",
    "        \n",
    "        # Show columns with high correlation (potential clustering candidates)\n",
    "        high_correlation = stats_df[abs(stats_df['correlation']) > 0.1].sort_values('correlation', ascending=False)\n",
    "        if len(high_correlation) > 0:\n",
    "            print(f\"  Columns with high correlation (clustering candidates):\")\n",
    "            for _, row in high_correlation.head().iterrows():\n",
    "                print(f\"    {row['tablename']}.{row['column_name']}: {row['correlation']:.3f} correlation\")\nexcept Exception as e:\n",
    "    print(f\"  Could not retrieve column statistics: {e}\")\n",
    "\n",
    "# Index analysis\n",
    "index_query = \"\"\"\n",
    "SELECT \n",
    "    schemaname,\n",
    "    tablename,\n",
    "    indexname,\n",
    "    indexdef\n",
    "FROM pg_indexes \n",
    "WHERE schemaname = 'public' \n",
    "AND tablename IN ('users', 'documents', 'conversations', 'messages')\n",
    "ORDER BY tablename, indexname\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    indexes_df = sync_execute_query(index_query)\n",
    "    \n",
    "    if len(indexes_df) > 0:\n",
    "        print(f\"\\nüóÇÔ∏è Current Indexes:\")\n",
    "        for table in indexes_df['tablename'].unique():\n",
    "            table_indexes = indexes_df[indexes_df['tablename'] == table]\n",
    "            print(f\"  {table}: {len(table_indexes)} indexes\")\n",
    "            for _, idx in table_indexes.iterrows():\n",
    "                # Extract index type and columns from definition\n",
    "                idx_def = idx['indexdef']\n",
    "                if 'UNIQUE' in idx_def:\n",
    "                    idx_type = \"UNIQUE\"\n",
    "                elif 'btree' in idx_def:\n",
    "                    idx_type = \"BTREE\"\n",
    "                else:\n",
    "                    idx_type = \"OTHER\"\n",
    "                print(f\"    {idx['indexname']}: {idx_type}\")\nexcept Exception as e:\n",
    "    print(f\"  Could not retrieve index information: {e}\")\n",
    "\n",
    "# Query performance recommendations\n",
    "print(f\"\\nüí° Performance Recommendations:\")\n",
    "\n",
    "# Based on data patterns\n",
    "if len(conversations_df) > 0:\n",
    "    avg_messages = conversations_df['message_count'].mean()\n",
    "    if avg_messages > 10:\n",
    "        print(f\"  - Consider partitioning messages table (avg {avg_messages:.1f} messages per conversation)\")\n",
    "\n",
    "if len(docs_df) > 0:\n",
    "    avg_word_count = docs_df['word_count'].mean()\n",
    "    if avg_word_count > 1000:\n",
    "        print(f\"  - Consider document chunking for large documents (avg {avg_word_count:.0f} words)\")\n",
    "\n",
    "# General recommendations based on data volume\n",
    "total_records = sum(stats.values())\n",
    "if total_records > 10000:\n",
    "    print(f\"  - Consider connection pooling for {total_records:,} total records\")\n",
    "    print(f\"  - Monitor query performance with pg_stat_statements\")\n",
    "    print(f\"  - Consider read replicas for heavy read workloads\")\n",
    "\n",
    "if len(messages_df) > 100:\n",
    "    print(f\"  - Consider archiving old messages ({len(messages_df)} total messages)\")\n",
    "    print(f\"  - Implement message pagination for large conversations\")\n",
    "\n",
    "print(f\"\\nüéØ Optimization Priorities:\")\n",
    "print(f\"  1. Index frequently queried columns (user_id, conversation_id)\")\n",
    "print(f\"  2. Monitor and optimize slow queries\")\n",
    "print(f\"  3. Implement appropriate caching strategies\")\n",
    "print(f\"  4. Consider data archiving for old records\")\n",
    "print(f\"  5. Set up monitoring and alerting for database performance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary and Action Items\n",
    "\n",
    "Based on this database exploration, here are the key findings and recommended actions:\n",
    "\n",
    "### üìä Data Overview\n",
    "- Database health and connection status\n",
    "- Record distribution across tables\n",
    "- Data quality assessment results\n",
    "\n",
    "### üîç Key Insights\n",
    "- User activity patterns and engagement levels\n",
    "- Document content analysis and categorization\n",
    "- Conversation and token usage patterns\n",
    "- Cross-table relationships and correlations\n",
    "\n",
    "### ‚ö†Ô∏è Quality Issues\n",
    "- Review any data quality issues identified\n",
    "- Implement data validation rules\n",
    "- Set up data quality monitoring\n",
    "\n",
    "### ‚ö° Performance Opportunities\n",
    "- Index optimization based on query patterns\n",
    "- Data archiving strategies for growth\n",
    "- Monitoring and alerting setup\n",
    "\n",
    "### üöÄ Next Steps\n",
    "1. **Performance Analysis**: Run `notebooks/evaluation/01_performance_analysis.ipynb`\n",
    "2. **User Behavior**: Explore `notebooks/exploration/02_user_behavior.ipynb`\n",
    "3. **Query Optimization**: Identify and optimize slow queries\n",
    "4. **Monitoring Setup**: Implement database monitoring and alerting\n",
    "\n",
    "This exploration provides a solid foundation for understanding your data and optimizing the Component Forge platform! üéØ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Component Forge",
   "language": "python",
   "name": "component-forge"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}